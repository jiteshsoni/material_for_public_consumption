#!/usr/bin/env python3
"""
Lakebase Benchmark Analysis & Visualization Tool
Generates engineering-focused visualizations and insights from benchmark results
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime
import os
import glob

def find_latest_results():
    """Find the most recent benchmark results"""
    # Check both old and new results directories
    from secure_config_finder import get_results_directory
    results_base = get_results_directory()
    results_dirs = glob.glob(f"{results_base}/*/")
    if not results_dirs:
        raise FileNotFoundError("No benchmark results found in results/ directory")
    
    latest_dir = max(results_dirs, key=os.path.getmtime)
    csv_files = glob.glob(f"{latest_dir}*.csv")
    
    if not csv_files:
        raise FileNotFoundError(f"No CSV files found in {latest_dir}")
    
    return csv_files[0], latest_dir

def load_benchmark_data(csv_path):
    """Load and prepare benchmark data"""
    df = pd.read_csv(csv_path)
    
    # Add derived metrics
    df['efficiency_score'] = df['throughput_ops_sec'] / df['threads']  # Queries per thread
    df['reliability_score'] = 100 - df['error_rate_pct']  # Inverse of error rate
    df['performance_score'] = df['throughput_ops_sec'] * (df['reliability_score'] / 100)  # Weighted performance
    
    return df

def create_performance_visualizations(df, output_dir):
    """Create comprehensive performance visualizations"""
    
    # Set style for professional plots
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # Create figure with subplots
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Primary Performance Metrics
    ax1 = plt.subplot(2, 3, 1)
    ax1_twin = ax1.twinx()
    
    # Throughput line
    line1 = ax1.plot(df['threads'], df['throughput_ops_sec'], 'o-', linewidth=3, markersize=8, 
                     color='#2E86AB', label='Throughput (queries/sec)')
    ax1.set_xlabel('Concurrent Connections (Threads)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Throughput (queries/sec)', fontsize=12, fontweight='bold', color='#2E86AB')
    ax1.tick_params(axis='y', labelcolor='#2E86AB')
    ax1.grid(True, alpha=0.3)
    
    # Error rate line
    line2 = ax1_twin.plot(df['threads'], df['error_rate_pct'], 's-', linewidth=3, markersize=8, 
                          color='#F18F01', label='Error Rate (%)')
    ax1_twin.set_ylabel('Error Rate (%)', fontsize=12, fontweight='bold', color='#F18F01')
    ax1_twin.tick_params(axis='y', labelcolor='#F18F01')
    
    # Highlight sweet spot (highest throughput with 0% errors)
    zero_error_df = df[df['error_rate_pct'] == 0.0]
    if not zero_error_df.empty:
        best_zero_error = zero_error_df.loc[zero_error_df['throughput_ops_sec'].idxmax()]
        ax1.annotate(f'Sweet Spot\n{best_zero_error["threads"]} threads\n{best_zero_error["throughput_ops_sec"]:.1f} q/s\n0% errors', 
                     xy=(best_zero_error['threads'], best_zero_error['throughput_ops_sec']),
                     xytext=(best_zero_error['threads']+10, best_zero_error['throughput_ops_sec']+200),
                     arrowprops=dict(arrowstyle='->', color='green', lw=2),
                     bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.7),
                     fontsize=10, fontweight='bold')
    
    ax1.set_title('üéØ Lakebase Performance: Throughput vs Error Rate', fontsize=14, fontweight='bold', pad=20)
    
    # Combine legends
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax1_twin.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='center left')
    
    # 2. Latency Analysis
    ax2 = plt.subplot(2, 3, 2)
    ax2.plot(df['threads'], df['avg_latency_ms'], 'o-', linewidth=2, label='Average Latency', color='#A23B72')
    ax2.plot(df['threads'], df['p95_latency_ms'], 's-', linewidth=2, label='P95 Latency', color='#F18F01')
    ax2.plot(df['threads'], df['p99_latency_ms'], '^-', linewidth=2, label='P99 Latency', color='#C73E1D')
    ax2.set_xlabel('Concurrent Connections (Threads)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Latency (milliseconds)', fontsize=12, fontweight='bold')
    ax2.set_title('‚è±Ô∏è Latency Scaling Pattern', fontsize=14, fontweight='bold', pad=20)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Efficiency Analysis (Queries per Thread)
    ax3 = plt.subplot(2, 3, 3)
    bars = ax3.bar(df['threads'], df['efficiency_score'], color='#2E86AB', alpha=0.7, edgecolor='black')
    ax3.set_xlabel('Concurrent Connections (Threads)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Efficiency (queries/sec per thread)', fontsize=12, fontweight='bold')
    ax3.set_title('‚ö° Thread Efficiency Analysis', fontsize=14, fontweight='bold', pad=20)
    ax3.grid(True, alpha=0.3, axis='y')
    
    # Highlight most efficient configuration
    max_efficiency_idx = df['efficiency_score'].idxmax()
    bars[max_efficiency_idx].set_color('#F18F01')
    ax3.annotate(f'Most Efficient\n{df.loc[max_efficiency_idx, "efficiency_score"]:.1f} q/s/thread', 
                 xy=(df.loc[max_efficiency_idx, 'threads'], df.loc[max_efficiency_idx, 'efficiency_score']),
                 xytext=(df.loc[max_efficiency_idx, 'threads'], df.loc[max_efficiency_idx, 'efficiency_score'] + 2),
                 ha='center', fontweight='bold',
                 bbox=dict(boxstyle="round,pad=0.3", facecolor='orange', alpha=0.7))
    
    # 4. OAuth Token Limit Analysis  
    ax4 = plt.subplot(2, 3, 4)
    
    # Create error threshold visualization
    colors = ['green' if err == 0 else 'orange' if err < 0.1 else 'red' for err in df['error_rate_pct']]
    scatter = ax4.scatter(df['threads'], df['throughput_ops_sec'], c=colors, s=200, alpha=0.7, edgecolors='black')
    
    # Add connection error counts as text
    for idx, row in df.iterrows():
        if row['connection_error_count'] > 0:
            ax4.annotate(f"{int(row['connection_error_count'])} conn errors", 
                        xy=(row['threads'], row['throughput_ops_sec']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=9, alpha=0.8)
    
    ax4.set_xlabel('Concurrent Connections (Threads)', fontsize=12, fontweight='bold')
    ax4.set_ylabel('Throughput (queries/sec)', fontsize=12, fontweight='bold')
    ax4.set_title('üîê OAuth Token Concurrency Limits', fontsize=14, fontweight='bold', pad=20)
    ax4.grid(True, alpha=0.3)
    
    # Add legend for colors
    from matplotlib.lines import Line2D
    legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='0% errors'),
                      Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='<0.1% errors'),
                      Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='‚â•0.1% errors')]
    ax4.legend(handles=legend_elements, loc='lower right')
    
    # 5. Performance Score Comparison
    ax5 = plt.subplot(2, 3, 5)
    bars = ax5.bar(df['threads'], df['performance_score'], color='#2E86AB', alpha=0.7, edgecolor='black')
    ax5.set_xlabel('Concurrent Connections (Threads)', fontsize=12, fontweight='bold')
    ax5.set_ylabel('Performance Score (throughput √ó reliability)', fontsize=12, fontweight='bold')
    ax5.set_title('üèÜ Overall Performance Score', fontsize=14, fontweight='bold', pad=20)
    ax5.grid(True, alpha=0.3, axis='y')
    
    # Highlight best overall performance
    max_score_idx = df['performance_score'].idxmax()
    bars[max_score_idx].set_color('#F18F01')
    ax5.annotate(f'Best Overall\n{df.loc[max_score_idx, "performance_score"]:.1f}', 
                 xy=(df.loc[max_score_idx, 'threads'], df.loc[max_score_idx, 'performance_score']),
                 xytext=(df.loc[max_score_idx, 'threads'], df.loc[max_score_idx, 'performance_score'] + 50),
                 ha='center', fontweight='bold',
                 bbox=dict(boxstyle="round,pad=0.3", facecolor='orange', alpha=0.7))
    
    # 6. Data Table Summary
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('tight')
    ax6.axis('off')
    
    # Create summary table
    summary_data = []
    for _, row in df.iterrows():
        summary_data.append([
            f"{int(row['threads'])}",
            f"{row['throughput_ops_sec']:.1f}",
            f"{row['avg_latency_ms']:.1f}ms",
            f"{row['error_rate_pct']:.2f}%",
            f"{int(row['connection_error_count'])}"
        ])
    
    table = ax6.table(cellText=summary_data,
                     colLabels=['Threads', 'Queries/sec', 'Avg Latency', 'Error Rate', 'Conn Errors'],
                     cellLoc='center',
                     loc='center',
                     bbox=[0, 0, 1, 1])
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # Color code the table rows
    for i in range(len(summary_data)):
        if df.iloc[i]['error_rate_pct'] == 0:
            color = 'lightgreen'
        elif df.iloc[i]['error_rate_pct'] < 0.1:
            color = 'lightyellow'  
        else:
            color = 'lightcoral'
        
        for j in range(len(summary_data[i])):
            table[(i+1, j)].set_facecolor(color)
    
    ax6.set_title('üìä Performance Summary Table', fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout(pad=3.0)
    
    # Save the plot
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    plot_path = os.path.join(output_dir, f'lakebase_performance_analysis_{timestamp}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
    
    print(f"üìä Performance visualizations saved: {plot_path}")
    return plot_path

def generate_engineering_analysis(df, output_dir):
    """Generate engineering-focused analysis document"""
    
    # Calculate key insights
    zero_error_configs = df[df['error_rate_pct'] == 0.0]
    best_zero_error = zero_error_configs.loc[zero_error_configs['throughput_ops_sec'].idxmax()] if not zero_error_configs.empty else None
    peak_throughput = df.loc[df['throughput_ops_sec'].idxmax()]
    most_efficient = df.loc[df['efficiency_score'].idxmax()]
    
    # OAuth token limit analysis
    error_configs_filtered = df[df['error_rate_pct'] > 0]
    first_error_config = error_configs_filtered.iloc[0] if not error_configs_filtered.empty else None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    analysis_path = os.path.join(output_dir, f'engineering_analysis_{timestamp}.md')
    
    with open(analysis_path, 'w') as f:
        f.write("# üî¨ Lakebase Performance Analysis for Engineering\n")
        f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Executive Summary
        f.write("## üéØ Executive Summary\n\n")
        f.write("Our Lakebase benchmark reveals **OAuth token concurrency limits** and optimal scaling patterns:\n\n")
        
        if best_zero_error is not None:
            f.write(f"- **Recommended Production Config:** {int(best_zero_error['threads'])} concurrent connections\n")
            f.write(f"- **Reliable Throughput:** {best_zero_error['throughput_ops_sec']:.1f} queries/sec with 0% errors\n")
        
        f.write(f"- **Peak Throughput:** {peak_throughput['throughput_ops_sec']:.1f} queries/sec at {int(peak_throughput['threads'])} threads\n")
        
        if first_error_config is not None:
            f.write(f"- **OAuth Token Limit:** Connection failures start at {int(first_error_config['threads'])} threads\n\n")
        else:
            f.write(f"- **OAuth Token Limit:** No connection failures detected in tested range\n\n")
        
        # Technical Deep Dive
        f.write("## üîß Technical Analysis\n\n")
        
        f.write("### OAuth Token Concurrency Limits\n")
        f.write("Our testing revealed **hard limits** in Databricks OAuth token concurrent connections:\n\n")
        
        error_configs = df[df['error_rate_pct'] > 0]
        if not error_configs.empty:
            f.write("| Threads | Throughput | Connection Errors | Error Rate |\n")
            f.write("|---------|------------|------------------|------------|\n")
            for _, row in error_configs.iterrows():
                f.write(f"| {int(row['threads'])} | {row['throughput_ops_sec']:.1f} q/s | {int(row['connection_error_count'])} | {row['error_rate_pct']:.2f}% |\n")
        
        f.write("\n**Key Finding:** OAuth tokens can reliably handle ~32 concurrent connections before authentication failures occur.\n\n")
        
        f.write("### Performance Scaling Pattern\n")
        f.write("Lakebase shows **excellent linear scaling** up to the OAuth limit:\n\n")
        
        f.write(f"- **Thread Efficiency Peak:** {most_efficient['efficiency_score']:.1f} queries/sec per thread at {int(most_efficient['threads'])} threads\n")
        f.write("- **Latency Stability:** Consistent sub-100ms P99 latency across all configurations\n")
        f.write("- **Error Pattern:** Connection failures are predictable and start at specific thresholds\n\n")
        
        # Architecture Implications
        f.write("## üèóÔ∏è Architecture Implications\n\n")
        
        f.write("### For Production Deployments:\n")
        f.write("```\n")
        if best_zero_error is not None:
            f.write(f"‚úÖ RECOMMENDED: {int(best_zero_error['threads'])} concurrent connections\n")
            f.write(f"   - Throughput: {best_zero_error['throughput_ops_sec']:.1f} queries/sec\n")
            f.write(f"   - Latency: {best_zero_error['avg_latency_ms']:.1f}ms avg, {best_zero_error['p99_latency_ms']:.1f}ms P99\n")
            f.write("   - Error Rate: 0.00% (100% reliable)\n")
        f.write("```\n\n")
        
        f.write("### For High-Throughput Scenarios:\n")
        f.write("```\n")
        f.write(f"‚ö†Ô∏è  MAXIMUM: {int(peak_throughput['threads'])} concurrent connections\n")
        f.write(f"   - Throughput: {peak_throughput['throughput_ops_sec']:.1f} queries/sec\n")
        f.write(f"   - Error Rate: {peak_throughput['error_rate_pct']:.2f}% ({int(peak_throughput['connection_error_count'])} connection failures)\n")
        f.write("   - Trade-off: Higher throughput with authentication failures\n")
        f.write("```\n\n")
        
        # Recommendations
        f.write("## üí° Engineering Recommendations\n\n")
        
        f.write("### 1. **Connection Pooling Strategy**\n")
        if best_zero_error is not None:
            f.write(f"- Set connection pool max size to **{int(best_zero_error['threads'])} connections**\n")
        f.write("- Implement connection retry logic for OAuth token refresh\n")
        f.write("- Monitor connection error rates as early warning system\n\n")
        
        f.write("### 2. **Application Scaling Patterns**\n")
        f.write("- **Scale horizontally** with multiple OAuth tokens rather than increasing connections per token\n")
        f.write("- **Load balancing:** Distribute requests across multiple Lakebase instances\n")
        f.write("- **Circuit breakers:** Implement backoff when connection errors exceed thresholds\n\n")
        
        f.write("### 3. **Monitoring & Alerting**\n")
        f.write("- **Alert on:** Connection error rate > 0.1%\n")
        f.write("- **Monitor:** Concurrent connection count approaching OAuth limits\n")
        f.write("- **Dashboard:** Track throughput, latency, and error rates continuously\n\n")
        
        # Performance Benchmarks
        f.write("## üìä Performance Benchmarks\n\n")
        f.write("| Configuration | Use Case | Throughput | Latency (P99) | Reliability |\n")
        f.write("|---------------|----------|------------|---------------|-------------|\n")
        
        if best_zero_error is not None:
            f.write(f"| {int(best_zero_error['threads'])} threads | **Production** | {best_zero_error['throughput_ops_sec']:.1f} q/s | {best_zero_error['p99_latency_ms']:.1f}ms | 100% |\n")
        
        f.write(f"| {int(peak_throughput['threads'])} threads | **Peak Load** | {peak_throughput['throughput_ops_sec']:.1f} q/s | {peak_throughput['p99_latency_ms']:.1f}ms | {100 - peak_throughput['error_rate_pct']:.1f}% |\n")
        f.write(f"| {int(most_efficient['threads'])} threads | **Efficient** | {most_efficient['throughput_ops_sec']:.1f} q/s | {most_efficient['p99_latency_ms']:.1f}ms | {100 - most_efficient['error_rate_pct']:.1f}% |\n\n")
        
        # Query Pattern Analysis  
        f.write("## üîç Query Pattern Analysis\n\n")
        f.write("**Point Lookup Queries Tested:**\n")
        f.write("1. `SELECT * FROM benchmark_test WHERE user_id = ?` (Full record retrieval)\n")
        f.write("2. `SELECT * FROM benchmark_test WHERE user_id = ? LIMIT 1` (Single record)\n")
        f.write("3. `SELECT user_id, amount, status FROM benchmark_test WHERE user_id = ?` (Specific columns)\n")
        f.write("4. `SELECT COUNT(*) FROM benchmark_test WHERE user_id = ?` (Aggregation)\n")
        f.write("5. `SELECT user_id, category, amount FROM benchmark_test WHERE user_id = ? LIMIT 5` (Limited results)\n\n")
        
        f.write("**Index Configuration:**\n")
        f.write("- Single B-tree index on `user_id` column\n")
        f.write("- Table size: 1,000,000 rows\n")
        f.write("- PostgreSQL 16.8 engine\n\n")
        
        # Next Steps
        f.write("## üöÄ Next Steps\n\n")
        f.write("1. **Validate OAuth token refresh patterns** in production workloads\n")
        f.write("2. **Test mixed query types** (not just point lookups)\n")
        f.write("3. **Benchmark write operations** (INSERT/UPDATE/DELETE)\n")
        f.write("4. **Evaluate multiple table scenarios** with JOIN operations\n")
        f.write("5. **Test connection pooling libraries** (HikariCP, pgpool-II)\n\n")
        
        f.write("---\n")
        f.write("*Analysis generated by Lakebase Benchmark Tool*\n")
    
    print(f"üìÑ Engineering analysis saved: {analysis_path}")
    return analysis_path

def main():
    """Main analysis function"""
    try:
        # Find latest results
        csv_path, results_dir = find_latest_results()
        print(f"üìÅ Analyzing results from: {csv_path}")
        
        # Load data
        df = load_benchmark_data(csv_path)
        print(f"üìä Loaded {len(df)} benchmark configurations")
        
        # Create output directory for analysis
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        analysis_dir = f"analysis_{timestamp}"
        os.makedirs(analysis_dir, exist_ok=True)
        
        # Generate visualizations
        plot_path = create_performance_visualizations(df, analysis_dir)
        
        # Generate engineering analysis
        analysis_path = generate_engineering_analysis(df, analysis_dir)
        
        print(f"\nüéâ Analysis Complete!")
        print(f"üìÅ Output folder: {analysis_dir}/")
        print(f"üìä Visualizations: {os.path.basename(plot_path)}")
        print(f"üìÑ Engineering report: {os.path.basename(analysis_path)}")
        
        return analysis_dir
        
    except Exception as e:
        print(f"‚ùå Analysis failed: {e}")
        return None

if __name__ == "__main__":
    main()