{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92f3a23-c265-4322-90d4-9a1cee0dc169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Table JSON Stats Inspector\n",
    "\n",
    "**Author:** Jitesh Soni \n",
    "**Date:** 2025-03-14\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook extracts column-level statistics from the Delta table's JSON log files stored in the `_delta_log` directory. The notebook performs the following tasks:\n",
    "- Retrieves the Delta table path using the Delta Lake API.\n",
    "- Reads the Delta log JSON files and filters out logs that contain column statistics.\n",
    "- Automatically infers the schema from a sample JSON string using DataFrame operations (avoiding custom RDD code) to comply with shared cluster limitations.\n",
    "- Parses and flattens the JSON statistics for further analysis.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Update the `table_name` variable with the full name of your Delta table.\n",
    "2. Run the cells sequentially to inspect the parsed column-level statistics.\n",
    "3. The final DataFrame shows the file paths along with parsed statistics such as `numRecords`, `minValues`, `maxValues`, and `nullCount`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b91343-f929-464a-b372-f723ac627fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Link To Demo Video](https://www.loom.com/share/001cfe9fc730481ea5b2a44c621c6b55?sid=03c724f5-e95a-42a1-838d-836eabbd65fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2c5dab-8874-4069-8c01-d9f95c4edafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType,\n",
    "    DoubleType, BooleanType, ArrayType\n",
    ")\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad078c6f-65a9-4f81-9c82-30ffe5a1b39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def infer_spark_schema_from_dict(d: dict) -> StructType:\n",
    "    \"\"\"\n",
    "    Recursively infers a Spark StructType schema from a Python dict.\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            field_type = infer_spark_schema_from_dict(v)\n",
    "        elif isinstance(v, list):\n",
    "            # For non-empty lists, infer type from first element; else default to StringType\n",
    "            if len(v) > 0:\n",
    "                first_elem = v[0]\n",
    "                if isinstance(first_elem, dict):\n",
    "                    element_type = infer_spark_schema_from_dict(first_elem)\n",
    "                elif isinstance(first_elem, int):\n",
    "                    element_type = LongType()\n",
    "                elif isinstance(first_elem, float):\n",
    "                    element_type = DoubleType()\n",
    "                elif isinstance(first_elem, bool):\n",
    "                    element_type = BooleanType()\n",
    "                else:\n",
    "                    element_type = StringType()\n",
    "            else:\n",
    "                element_type = StringType()\n",
    "            field_type = ArrayType(element_type)\n",
    "        elif isinstance(v, int):\n",
    "            field_type = LongType()\n",
    "        elif isinstance(v, float):\n",
    "            field_type = DoubleType()\n",
    "        elif isinstance(v, bool):\n",
    "            field_type = BooleanType()\n",
    "        else:\n",
    "            field_type = StringType()\n",
    "        fields.append(StructField(k, field_type, True))\n",
    "    return StructType(fields)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02da80a-f979-440f-bc57-5d37eb95fe5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def inspect_column_stats_from_delta_table(table_name: str, spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Extracts column-level statistics from Delta log JSON files for a given table,\n",
    "    and automatically infers the schema of the JSON stats using a sample row.\n",
    "    \n",
    "    Args:\n",
    "        table_name (str): The full name of the Delta table (e.g., \"db_name.table_name\").\n",
    "        spark (SparkSession): Your active SparkSession.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with the parsed column statistics.\n",
    "    \"\"\"\n",
    "    # Get Delta table path\n",
    "    delta_table_path = (\n",
    "        DeltaTable.forName(spark, table_name)\n",
    "        .detail()\n",
    "        .select(\"location\")\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    print(f\"üîç Delta Table Path: {delta_table_path}\")\n",
    "\n",
    "    # Read the delta log JSON files\n",
    "    log_path = f\"{delta_table_path}/_delta_log/*.json\"\n",
    "    logs = spark.read.json(log_path).cache()\n",
    "\n",
    "    # Filter logs with stats and select relevant columns\n",
    "    relevant_logs = logs.filter(col(\"add.stats\").isNotNull())\n",
    "    stats_logs = relevant_logs.select(col(\"add.path\").alias(\"path\"), col(\"add.stats\").alias(\"stats\"))\n",
    "\n",
    "    # Get one sample JSON string from the stats column (driver-side collection of one row only)\n",
    "    sample_row = stats_logs.filter(col(\"stats\").isNotNull()).limit(1).collect()\n",
    "    if not sample_row:\n",
    "        raise ValueError(\"No stats found in the logs.\")\n",
    "    sample_json_str = sample_row[0][\"stats\"]\n",
    "\n",
    "    # Convert the sample JSON string into a dict\n",
    "    sample_dict = json.loads(sample_json_str)\n",
    "\n",
    "    # Infer the schema from the sample dictionary\n",
    "    inferred_schema = infer_spark_schema_from_dict(sample_dict)\n",
    "    print(\"Inferred Schema:\")\n",
    "    print(inferred_schema.json())\n",
    "\n",
    "    # Parse the stats column using the inferred schema\n",
    "    stats_with_schema = stats_logs.withColumn(\"parsed_stats\", from_json(col(\"stats\"), inferred_schema))\n",
    "\n",
    "    # Flatten the parsed stats into individual columns (along with the file path)\n",
    "    final_df = stats_with_schema.select(\"path\", \"parsed_stats.*\")\n",
    "    display(final_df)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b011fb-b1a1-4c36-9707-adbc4d709d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "table_name = \"soni.default.iot_data_merge_partitioned\"\n",
    "column_stats_df = inspect_column_stats_from_delta_table(table_name, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3df4b8-65e2-4daf-9447-b3eed98f1f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "delta_table_stats_inspector",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
