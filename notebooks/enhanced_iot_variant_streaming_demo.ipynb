{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c50d2de-f985-4c2e-9103-9c1622b9268f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Enhanced IoT Sensor Data Demo with VARIANT Type\n",
    "\n",
    "This notebook demonstrates advanced IoT sensor data streaming with VARIANT columns in Databricks:\n",
    "\n",
    "## Features\n",
    "- **VARIANT Column Support**: Store complex nested JSON metadata\n",
    "- **Production-Ready Streaming**: Robust error handling and monitoring\n",
    "- **Configurable Data Generation**: Parameterized synthetic data creation\n",
    "- **Real-Time Analytics**: Advanced querying of VARIANT data\n",
    "- **Databricks Cluster Optimized**: Designed for remote cluster execution\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 13.3 LTS or higher\n",
    "- Unity Catalog enabled workspace with volume access\n",
    "- Cluster with appropriate permissions for streaming and Delta operations\n",
    "\n",
    "## Architecture\n",
    "- Streaming source ‚Üí Delta table with VARIANT columns ‚Üí Real-time analytics\n",
    "- Automatic schema evolution and checkpoint management\n",
    "- Performance optimized with configurable partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b30d5bc-e90d-45d9-bb35-858235e96233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install packages and restart Python runtime\n",
    "%pip install faker\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9687075-89ba-404d-bb2b-30d7722622aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Use standard SparkSession for Databricks cluster execution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import (\n",
    "    expr, when, col, concat, lit, to_json, struct, current_timestamp, \n",
    "    parse_json, floor, rand, count, avg, max as sql_max, array\n",
    ")\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Configure logging for Databricks\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use existing Spark session available in Databricks cluster\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "print(\"‚úÖ All imports completed successfully\")\n",
    "print(f\"üî• Spark version: {spark.version}\")\n",
    "print(f\"üöÄ Running on Databricks cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5317e823-7b83-4c46-8b05-1df6e1b06045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration for Databricks cluster execution\n",
    "@dataclass\n",
    "class StreamingConfig:\n",
    "    \"\"\"Configuration for IoT streaming demo on Databricks cluster\"\"\"\n",
    "    partitions: int = 8\n",
    "    rows_per_second: int = 50_000  # Reduced for cluster stability\n",
    "    catalog: str = \"soni\"\n",
    "    database: str = \"default\"\n",
    "    table_name: str = f\"iot_sensor_variant_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    checkpoint_base_path: str = \"/Volumes/soni/default/checkpoints\"  # Using Unity Catalog Volumes\n",
    "    \n",
    "    @property\n",
    "    def full_table_name(self) -> str:\n",
    "        return f\"{self.catalog}.{self.database}.{self.table_name}\"\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_path(self) -> str:\n",
    "        return f\"{self.checkpoint_base_path}/iot_variant_{uuid.uuid4()}\"\n",
    "\n",
    "class IoTSensorDataGenerator:\n",
    "    \"\"\"Enhanced IoT sensor data generator optimized for Databricks cluster\"\"\"\n",
    "    \n",
    "    def __init__(self, config: StreamingConfig):\n",
    "        self.config = config\n",
    "        self.spark = spark  # Use the global spark session\n",
    "        \n",
    "    def create_table_if_not_exists(self) -> None:\n",
    "        \"\"\"Create Delta table with VARIANT column and optimizations\"\"\"\n",
    "        try:\n",
    "            # Check if catalog/database exists - handle permissions gracefully\n",
    "            try:\n",
    "                spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.config.catalog}\")\n",
    "                print(f\"‚úÖ Catalog {self.config.catalog} verified/created\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Catalog creation note: {e}\")\n",
    "                print(f\"üí° Proceeding with existing catalog permissions\")\n",
    "            \n",
    "            try:\n",
    "                spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.config.catalog}.{self.config.database}\")\n",
    "                print(f\"‚úÖ Database {self.config.catalog}.{self.config.database} verified/created\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Database creation note: {e}\")\n",
    "                print(f\"üí° Proceeding with existing database permissions\")\n",
    "            \n",
    "            create_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.config.full_table_name} (\n",
    "                sensor_id STRING,\n",
    "                location STRING,\n",
    "                temperature DOUBLE,\n",
    "                humidity INTEGER,\n",
    "                sensor_metadata VARIANT,\n",
    "                reading_timestamp TIMESTAMP,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (DATE(reading_timestamp))\n",
    "            TBLPROPERTIES (\n",
    "                'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "                'delta.autoOptimize.autoCompact' = 'true',\n",
    "                'delta.enableChangeDataFeed' = 'true'\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            self.spark.sql(create_sql)\n",
    "            print(f\"‚úÖ Table {self.config.full_table_name} created/verified with optimizations\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create table: {e}\")\n",
    "            print(f\"üí° Check Unity Catalog permissions for catalog: {self.config.catalog}\")\n",
    "            raise\n",
    "\n",
    "# Initialize configuration and generator\n",
    "config = StreamingConfig()\n",
    "generator = IoTSensorDataGenerator(config)\n",
    "\n",
    "print(f\"‚úÖ Configuration initialized for Databricks cluster:\")\n",
    "print(f\"   - Table: {config.full_table_name}\")\n",
    "print(f\"   - Partitions: {config.partitions}\")\n",
    "print(f\"   - Rows/second: {config.rows_per_second:,}\")\n",
    "print(f\"   - Checkpoint: {config.checkpoint_path}\")\n",
    "print(f\"üîß Using Unity Catalog Volumes for checkpoints\")\n",
    "print(f\"üí° Ensure proper permissions for catalog '{config.catalog}' and volume access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930f4c3d-e8eb-4963-b067-eec8bfdc65ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create table and setup streaming data WITHOUT dbldatagen dependency\n",
    "generator.create_table_if_not_exists()\n",
    "\n",
    "print(\"üîß Creating streaming data source using native PySpark (no dbldatagen dependency)\")\n",
    "\n",
    "# Create streaming DataFrame using native PySpark rate source instead of dbldatagen\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", config.rows_per_second)\n",
    "    .option(\"numPartitions\", config.partitions)\n",
    "    .load()\n",
    "    .select(\n",
    "        # Generate sensor data from rate stream\n",
    "        (lit(\"SENSOR_\") + (col(\"value\") % 10000).cast(\"string\")).alias(\"sensor_id\"),\n",
    "        (array(lit(\"Building_A\"), lit(\"Building_B\"), lit(\"Building_C\"), lit(\"Warehouse\"), lit(\"DataCenter\"))\n",
    "         .getItem((col(\"value\") % 5).cast(\"int\"))).alias(\"location\"),\n",
    "        (rand() * 55 - 10).alias(\"temperature\"),  # -10 to 45 degrees\n",
    "        (floor(rand() * 70) + 30).cast(\"int\").alias(\"humidity\"),  # 30-90% humidity\n",
    "        \n",
    "        # Create rich VARIANT metadata using native PySpark functions\n",
    "        parse_json(to_json(struct(\n",
    "            # Core sensor metrics\n",
    "            (floor(rand() * 100) + 1).cast(\"int\").alias(\"battery_level\"),\n",
    "            (floor(rand() * 100) - 100).cast(\"int\").alias(\"signal_strength\"),\n",
    "            (when(rand() < 0.8, \"OK\")\n",
    "             .when(rand() < 0.9, \"SENSOR_FAIL\")\n",
    "             .when(rand() < 0.95, \"BATTERY_LOW\")\n",
    "             .otherwise(\"COMM_LOSS\")).alias(\"status\"),\n",
    "            \n",
    "            # Maintenance and calibration data\n",
    "            current_timestamp().alias(\"last_maintenance\"),\n",
    "            lit(\"v2.1.5\").alias(\"firmware_version\"),\n",
    "            (floor(rand() * 1000) + 1).cast(\"int\").alias(\"calibration_count\"),\n",
    "            \n",
    "            # Device information\n",
    "            struct(\n",
    "                lit(\"Acme Corp\").alias(\"manufacturer\"),\n",
    "                lit(\"TempSense Pro\").alias(\"model\"),\n",
    "                lit(\"2023\").alias(\"year\"),\n",
    "                lit(\"TS-300X\").alias(\"part_number\")\n",
    "            ).alias(\"device_info\"),\n",
    "            \n",
    "            # Network and connectivity\n",
    "            struct(\n",
    "                lit(\"WiFi\").alias(\"connection_type\"),\n",
    "                (floor(rand() * 100) + 1).cast(\"int\").alias(\"network_id\"),\n",
    "                concat(lit(\"192.168.1.\"), (floor(rand() * 254) + 1).cast(\"string\")).alias(\"ip_address\")\n",
    "            ).alias(\"network\"),\n",
    "            \n",
    "            # Environmental conditions\n",
    "            struct(\n",
    "                (rand() * 10 + 20).alias(\"ambient_temp\"),\n",
    "                (rand() * 30 + 40).alias(\"ambient_humidity\"),\n",
    "                (rand() * 200 + 800).alias(\"pressure_hpa\")\n",
    "            ).alias(\"environment\")\n",
    "        ))).alias(\"sensor_metadata\"),\n",
    "        current_timestamp().alias(\"reading_timestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Native PySpark streaming DataFrame with rich VARIANT metadata created\")\n",
    "print(\"üìä VARIANT column includes: battery, signal, status, maintenance, device_info, network, environment\")\n",
    "print(\"üîß No external dependencies - uses only native Spark functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e3398b-6dda-4128-b233-313fab99dead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class StreamingQueryManager:\n",
    "    \"\"\"Manages streaming queries with enhanced monitoring for Databricks cluster\"\"\"\n",
    "    \n",
    "    def __init__(self, config: StreamingConfig):\n",
    "        self.config = config\n",
    "        self.query: Optional[StreamingQuery] = None\n",
    "        \n",
    "    def start_streaming(self, streaming_df) -> StreamingQuery:\n",
    "        \"\"\"Start streaming optimized for Databricks cluster\"\"\"\n",
    "        try:\n",
    "            # Enable adaptive query execution for better performance\n",
    "            spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            \n",
    "            self.query = (\n",
    "                streaming_df.writeStream\n",
    "                .queryName(\"enhanced_iot_variant_sensor_stream\")\n",
    "                .outputMode(\"append\")\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", self.config.checkpoint_path)\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .trigger(processingTime=\"30 seconds\")  # Process every 30 seconds\n",
    "                .toTable(self.config.full_table_name)\n",
    "            )\n",
    "            \n",
    "            print(f\"üöÄ Streaming query started on Databricks cluster: {self.query.name}\")\n",
    "            print(f\"üìç Checkpoint location: {self.config.checkpoint_path}\")\n",
    "            return self.query\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to start streaming: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_streaming_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed streaming status with cluster metrics\"\"\"\n",
    "        if not self.query:\n",
    "            return {\"status\": \"Not started\"}\n",
    "        \n",
    "        try:\n",
    "            progress = self.query.lastProgress\n",
    "            return {\n",
    "                \"query_name\": self.query.name,\n",
    "                \"status\": \"Running\" if self.query.isActive else \"Stopped\",\n",
    "                \"batch_id\": progress.get(\"batchId\", \"N/A\") if progress else \"N/A\",\n",
    "                \"input_rows_per_second\": progress.get(\"inputRowsPerSecond\", 0) if progress else 0,\n",
    "                \"processed_rows_per_second\": progress.get(\"processedRowsPerSecond\", 0) if progress else 0,\n",
    "                \"batch_duration_ms\": progress.get(\"batchDuration\", 0) if progress else 0,\n",
    "                \"timestamp\": progress.get(\"timestamp\") if progress else \"N/A\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting streaming status: {e}\")\n",
    "            return {\"status\": \"Error\", \"error\": str(e)}\n",
    "\n",
    "# Start streaming with proper testing pattern\n",
    "query_manager = StreamingQueryManager(config)\n",
    "\n",
    "# Use trigger(once=True) for initial data load, then switch to continuous streaming for testing\n",
    "print(\"üöÄ Starting initial data load with trigger(once=True)...\")\n",
    "initial_query = (\n",
    "    streaming_df.writeStream\n",
    "    .queryName(\"initial_iot_variant_load\")\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{config.checkpoint_path}_initial\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(once=True)\n",
    "    .toTable(config.full_table_name)\n",
    ")\n",
    "\n",
    "# Wait for initial load to complete\n",
    "initial_query.awaitTermination()\n",
    "print(\"‚úÖ Initial data load completed\")\n",
    "\n",
    "# Now start continuous streaming for ongoing data\n",
    "streaming_query = query_manager.start_streaming(streaming_df)\n",
    "\n",
    "print(f\"üöÄ Enhanced streaming started on Databricks cluster!\")\n",
    "print(f\"üìä Query: {streaming_query.name}\")\n",
    "print(f\"üîÑ Processing trigger: Every 30 seconds\")\n",
    "print(f\"üìà Target rate: {config.rows_per_second:,} rows/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bcd8a0-254b-42de-8d8e-f883cb428ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple VARIANT Column Testing - Just 2 Basic Test Cases\n",
    "\n",
    "import time\n",
    "\n",
    "# Allow streaming to run for a test period, then stop for validation\n",
    "print(\"‚è≥ Allowing streaming to process data for 60 seconds...\")\n",
    "time.sleep(60)\n",
    "\n",
    "# Stop streaming programmatically before running tests\n",
    "print(\"üõë Stopping streaming query for validation tests...\")\n",
    "if streaming_query and streaming_query.isActive:\n",
    "    streaming_query.stop()\n",
    "    print(\"‚úÖ Streaming query stopped for testing\")\n",
    "\n",
    "# Wait a moment for the stop to complete\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"\\n=== Simple VARIANT Column Tests ===\")\n",
    "\n",
    "# Test Case 1: Basic VARIANT field extraction\n",
    "print(\"\\n1Ô∏è‚É£ Test Case 1: Basic VARIANT Field Extraction\")\n",
    "basic_test = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    location,\n",
    "    temperature,\n",
    "    sensor_metadata:battery_level::INT as battery_level,\n",
    "    sensor_metadata:status::STRING as sensor_status,\n",
    "    sensor_metadata:firmware_version::STRING as firmware_version,\n",
    "    reading_timestamp\n",
    "FROM {config.full_table_name}\n",
    "ORDER BY reading_timestamp DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Basic VARIANT extraction successful:\")\n",
    "basic_test.show(truncate=False)\n",
    "\n",
    "# Test Case 2: Nested VARIANT object access\n",
    "print(\"\\n2Ô∏è‚É£ Test Case 2: Nested VARIANT Object Access\")\n",
    "nested_test = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    sensor_metadata:device_info.manufacturer::STRING as manufacturer,\n",
    "    sensor_metadata:device_info.model::STRING as model,\n",
    "    sensor_metadata:device_info.year::STRING as year,\n",
    "    sensor_metadata:network.connection_type::STRING as connection_type,\n",
    "    sensor_metadata:network.ip_address::STRING as ip_address,\n",
    "    sensor_metadata:environment.ambient_temp::DOUBLE as ambient_temp\n",
    "FROM {config.full_table_name}\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Nested VARIANT object access successful:\")\n",
    "nested_test.show(truncate=False)\n",
    "\n",
    "print(\"\\nüéØ VARIANT Column Parsing: FULLY VALIDATED!\")\n",
    "print(\"‚úÖ Both test cases demonstrate successful VARIANT column functionality\")\n",
    "print(\"üìä Ready for production streaming with complex JSON metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543b6c3c-7c49-4ac7-93ed-0faa75a32662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup and Summary\n",
    "\n",
    "print(\"üßπ Cleaning up streaming resources...\")\n",
    "\n",
    "# Stop the streaming query if it's still running\n",
    "if streaming_query and streaming_query.isActive:\n",
    "    streaming_query.stop()\n",
    "    print(\"‚úÖ Streaming query stopped\")\n",
    "\n",
    "# Show final summary\n",
    "print(f\"\\nüìã Demo Summary:\")\n",
    "print(f\"   - Table: {config.full_table_name}\")\n",
    "print(f\"   - VARIANT column: sensor_metadata\")\n",
    "print(f\"   - Streaming rate: {config.rows_per_second:,} rows/second\")\n",
    "print(f\"   - Checkpoint: {config.checkpoint_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Enhanced IoT VARIANT Streaming Demo completed successfully!\")\n",
    "print(f\"üí° Key achievements:\")\n",
    "print(f\"   ‚úÖ VARIANT column creation and population\")\n",
    "print(f\"   ‚úÖ Basic VARIANT field extraction (battery_level, status, firmware)\")\n",
    "print(f\"   ‚úÖ Nested VARIANT object access (device_info, network, environment)\")\n",
    "print(f\"   ‚úÖ Streaming data ingestion with VARIANT metadata\")\n",
    "print(f\"   ‚úÖ Production-ready error handling and monitoring\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for production deployment on Databricks clusters!\")\n",
    "\n",
    "# Optional: Show table info\n",
    "try:\n",
    "    row_count = spark.sql(f\"SELECT COUNT(*) as count FROM {config.full_table_name}\").collect()[0].count\n",
    "    print(f\"üìä Final table contains {row_count:,} rows with VARIANT data\")\n",
    "except:\n",
    "    print(\"üìä Table ready for production use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tq8m6jmt3h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced IoT VARIANT streaming demo on Databricks cluster\n",
    "print(\"üîç Testing enhanced_iot_variant_streaming_demo.ipynb on Databricks cluster...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Install packages and restart Python runtime\n",
    "print(\"üì¶ Installing faker package...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xm5zbdtdcsq",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Testing enhanced_iot_variant_streaming_demo.ipynb on Databricks cluster...\")\n",
    "print(\"Starting test execution...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ldtdbp54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports completed successfully\n",
      "üî• Spark version: 3.5.2\n",
      "üöÄ Running on Databricks cluster\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Use standard SparkSession for Databricks cluster execution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import (\n",
    "    expr, when, col, concat, lit, to_json, struct, current_timestamp, \n",
    "    parse_json, floor, rand, count, avg, max as sql_max, array\n",
    ")\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Configure logging for Databricks\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use existing Spark session available in Databricks cluster\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "print(\"‚úÖ All imports completed successfully\")\n",
    "print(f\"üî• Spark version: {spark.version}\")\n",
    "print(f\"üöÄ Running on Databricks cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc1wxud8ul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initialized for Databricks cluster:\n",
      "   - Table: soni.default.iot_sensor_variant_demo_20250821_235818\n",
      "   - Partitions: 8\n",
      "   - Rows/second: 50,000\n",
      "   - Checkpoint: /Volumes/soni/default/checkpoints/iot_variant_877f1222-c4fc-46d3-846b-dfd86f3e2429\n",
      "üîß Using Unity Catalog Volumes for checkpoints\n",
      "üí° Ensure proper permissions for catalog 'soni' and volume access\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Databricks cluster execution\n",
    "@dataclass\n",
    "class StreamingConfig:\n",
    "    \"\"\"Configuration for IoT streaming demo on Databricks cluster\"\"\"\n",
    "    partitions: int = 8\n",
    "    rows_per_second: int = 50_000  # Reduced for cluster stability\n",
    "    catalog: str = \"soni\"\n",
    "    database: str = \"default\"\n",
    "    table_name: str = f\"iot_sensor_variant_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    checkpoint_base_path: str = \"/Volumes/soni/default/checkpoints\"  # Using Unity Catalog Volumes\n",
    "    \n",
    "    @property\n",
    "    def full_table_name(self) -> str:\n",
    "        return f\"{self.catalog}.{self.database}.{self.table_name}\"\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_path(self) -> str:\n",
    "        return f\"{self.checkpoint_base_path}/iot_variant_{uuid.uuid4()}\"\n",
    "\n",
    "class IoTSensorDataGenerator:\n",
    "    \"\"\"Enhanced IoT sensor data generator optimized for Databricks cluster\"\"\"\n",
    "    \n",
    "    def __init__(self, config: StreamingConfig):\n",
    "        self.config = config\n",
    "        self.spark = spark  # Use the global spark session\n",
    "        \n",
    "    def create_table_if_not_exists(self) -> None:\n",
    "        \"\"\"Create Delta table with VARIANT column and optimizations\"\"\"\n",
    "        try:\n",
    "            # Check if catalog/database exists - handle permissions gracefully\n",
    "            try:\n",
    "                spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.config.catalog}\")\n",
    "                print(f\"‚úÖ Catalog {self.config.catalog} verified/created\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Catalog creation note: {e}\")\n",
    "                print(f\"üí° Proceeding with existing catalog permissions\")\n",
    "            \n",
    "            try:\n",
    "                spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.config.catalog}.{self.config.database}\")\n",
    "                print(f\"‚úÖ Database {self.config.catalog}.{self.config.database} verified/created\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Database creation note: {e}\")\n",
    "                print(f\"üí° Proceeding with existing database permissions\")\n",
    "            \n",
    "            create_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.config.full_table_name} (\n",
    "                sensor_id STRING,\n",
    "                location STRING,\n",
    "                temperature DOUBLE,\n",
    "                humidity INTEGER,\n",
    "                sensor_metadata VARIANT,\n",
    "                reading_timestamp TIMESTAMP,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    "            )\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (DATE(reading_timestamp))\n",
    "            TBLPROPERTIES (\n",
    "                'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "                'delta.autoOptimize.autoCompact' = 'true',\n",
    "                'delta.enableChangeDataFeed' = 'true'\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            self.spark.sql(create_sql)\n",
    "            print(f\"‚úÖ Table {self.config.full_table_name} created/verified with optimizations\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create table: {e}\")\n",
    "            print(f\"üí° Check Unity Catalog permissions for catalog: {self.config.catalog}\")\n",
    "            raise\n",
    "\n",
    "# Initialize configuration and generator\n",
    "config = StreamingConfig()\n",
    "generator = IoTSensorDataGenerator(config)\n",
    "\n",
    "print(f\"‚úÖ Configuration initialized for Databricks cluster:\")\n",
    "print(f\"   - Table: {config.full_table_name}\")\n",
    "print(f\"   - Partitions: {config.partitions}\")\n",
    "print(f\"   - Rows/second: {config.rows_per_second:,}\")\n",
    "print(f\"   - Checkpoint: {config.checkpoint_path}\")\n",
    "print(f\"üîß Using Unity Catalog Volumes for checkpoints\")\n",
    "print(f\"üí° Ensure proper permissions for catalog '{config.catalog}' and volume access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7hr9qmq3gph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jitesh.soni/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2394: UserWarning: Spark Connect Session expired on the server. Please generate a new session by detaching and reattaching the compute if in a Databricks notebook or job or by calling DatabricksSession.builder.getOrCreate() if using Databricks Connect.\n",
      "  warnings.warn(\n",
      "/Users/jitesh.soni/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py:255: UserWarning: ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\"[INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\"}\"\n",
      ">.\n",
      "  warnings.warn(f\"ReleaseExecute failed with exception: {e}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Catalog creation note: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\n",
      "üí° Proceeding with existing catalog permissions\n",
      "‚ö†Ô∏è  Database creation note: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\n",
      "üí° Proceeding with existing database permissions\n",
      "‚ùå Failed to create table: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\n",
      "üí° Check Unity Catalog permissions for catalog: soni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jitesh.soni/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py:255: UserWarning: ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000\", grpc_status:13}\"\n",
      ">.\n",
      "  warnings.warn(f\"ReleaseExecute failed with exception: {e}.\")\n"
     ]
    },
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "(org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSparkConnectGrpcException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create table and setup streaming data WITHOUT dbldatagen dependency\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_table_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîß Creating streaming data source using native PySpark (no dbldatagen dependency)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create streaming DataFrame using native PySpark rate source instead of dbldatagen\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mIoTSensorDataGenerator.create_table_if_not_exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müí° Proceeding with existing database permissions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m     create_sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[33m    CREATE TABLE IF NOT EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.full_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[33m        sensor_id STRING,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m \u001b[33m    )\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.full_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m created/verified with optimizations\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:821\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m         _views.append(SubqueryAlias(df._plan, name))\n\u001b[32m    820\u001b[39m cmd = SQL(sqlQuery, _args, _named_args, _views)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m data, properties, ei = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[32m    823\u001b[39m     df = DataFrame(CachedRelation(properties[\u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1481\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations, extra_request_metadata)\u001b[39m\n\u001b[32m   1479\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1480\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1481\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1485\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1970\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[39m\n\u001b[32m   1967\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1969\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1970\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1946\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, progress)\u001b[39m\n\u001b[32m   1944\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1945\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1946\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2264\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2347\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2343\u001b[39m             logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived ErrorInfo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2345\u001b[39m             \u001b[38;5;28mself\u001b[39m._handle_rpc_error_with_error_info(status, info)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2348\u001b[39m                 info,\n\u001b[32m   2349\u001b[39m                 status.message,\n\u001b[32m   2350\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2351\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2352\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[32m   2355\u001b[39m         message=status.message,\n\u001b[32m   2356\u001b[39m         sql_state=SparkConnectGrpcException.CLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m   2357\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mSparkConnectGrpcException\u001b[39m: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle cc5b0d54-9aa9-45a9-86a1-ecd600314a3f is invalid. Session was closed. SQLSTATE: HY000"
     ]
    }
   ],
   "source": [
    "# Create table and setup streaming data WITHOUT dbldatagen dependency\n",
    "generator.create_table_if_not_exists()\n",
    "\n",
    "print(\"üîß Creating streaming data source using native PySpark (no dbldatagen dependency)\")\n",
    "\n",
    "# Create streaming DataFrame using native PySpark rate source instead of dbldatagen\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", config.rows_per_second)\n",
    "    .option(\"numPartitions\", config.partitions)\n",
    "    .load()\n",
    "    .select(\n",
    "        # Generate sensor data from rate stream\n",
    "        (lit(\"SENSOR_\") + (col(\"value\") % 10000).cast(\"string\")).alias(\"sensor_id\"),\n",
    "        (array(lit(\"Building_A\"), lit(\"Building_B\"), lit(\"Building_C\"), lit(\"Warehouse\"), lit(\"DataCenter\"))\n",
    "         .getItem((col(\"value\") % 5).cast(\"int\"))).alias(\"location\"),\n",
    "        (rand() * 55 - 10).alias(\"temperature\"),  # -10 to 45 degrees\n",
    "        (floor(rand() * 70) + 30).cast(\"int\").alias(\"humidity\"),  # 30-90% humidity\n",
    "        \n",
    "        # Create rich VARIANT metadata using native PySpark functions\n",
    "        parse_json(to_json(struct(\n",
    "            # Core sensor metrics\n",
    "            (floor(rand() * 100) + 1).cast(\"int\").alias(\"battery_level\"),\n",
    "            (floor(rand() * 100) - 100).cast(\"int\").alias(\"signal_strength\"),\n",
    "            (when(rand() < 0.8, \"OK\")\n",
    "             .when(rand() < 0.9, \"SENSOR_FAIL\")\n",
    "             .when(rand() < 0.95, \"BATTERY_LOW\")\n",
    "             .otherwise(\"COMM_LOSS\")).alias(\"status\"),\n",
    "            \n",
    "            # Maintenance and calibration data\n",
    "            current_timestamp().alias(\"last_maintenance\"),\n",
    "            lit(\"v2.1.5\").alias(\"firmware_version\"),\n",
    "            (floor(rand() * 1000) + 1).cast(\"int\").alias(\"calibration_count\"),\n",
    "            \n",
    "            # Device information\n",
    "            struct(\n",
    "                lit(\"Acme Corp\").alias(\"manufacturer\"),\n",
    "                lit(\"TempSense Pro\").alias(\"model\"),\n",
    "                lit(\"2023\").alias(\"year\"),\n",
    "                lit(\"TS-300X\").alias(\"part_number\")\n",
    "            ).alias(\"device_info\"),\n",
    "            \n",
    "            # Network and connectivity\n",
    "            struct(\n",
    "                lit(\"WiFi\").alias(\"connection_type\"),\n",
    "                (floor(rand() * 100) + 1).cast(\"int\").alias(\"network_id\"),\n",
    "                concat(lit(\"192.168.1.\"), (floor(rand() * 254) + 1).cast(\"string\")).alias(\"ip_address\")\n",
    "            ).alias(\"network\"),\n",
    "            \n",
    "            # Environmental conditions\n",
    "            struct(\n",
    "                (rand() * 10 + 20).alias(\"ambient_temp\"),\n",
    "                (rand() * 30 + 40).alias(\"ambient_humidity\"),\n",
    "                (rand() * 200 + 800).alias(\"pressure_hpa\")\n",
    "            ).alias(\"environment\")\n",
    "        ))).alias(\"sensor_metadata\"),\n",
    "        current_timestamp().alias(\"reading_timestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Native PySpark streaming DataFrame with rich VARIANT metadata created\")\n",
    "print(\"üìä VARIANT column includes: battery, signal, status, maintenance, device_info, network, environment\")\n",
    "print(\"üîß No external dependencies - uses only native Spark functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49k21hk6qfp",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 23:58:54,985 - INFO - loading DEFAULT profile from ~/.databrickscfg: host, token, cluster_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Reconnected to Databricks cluster\n",
      "üî• Spark version: 3.5.2\n",
      "‚úÖ Basic connectivity test: Created range(10), count = 10\n",
      "üìù Testing table creation: soni.default.test_variant_20250821_235859\n",
      "‚úÖ VARIANT table created successfully: soni.default.test_variant_20250821_235859\n",
      "‚úÖ VARIANT data inserted successfully\n",
      "+---+----+-----+\n",
      "| id|name|value|\n",
      "+---+----+-----+\n",
      "|  1|test|  123|\n",
      "+---+----+-----+\n",
      "\n",
      "‚úÖ VARIANT data queried successfully\n"
     ]
    }
   ],
   "source": [
    "# Reconnect to Databricks cluster\n",
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "print(\"üîó Reconnected to Databricks cluster\")\n",
    "print(f\"üî• Spark version: {spark.version}\")\n",
    "\n",
    "# Test basic functionality\n",
    "df = spark.range(10)\n",
    "count = df.count()\n",
    "print(f\"‚úÖ Basic connectivity test: Created range(10), count = {count}\")\n",
    "\n",
    "# Test table creation with simpler approach\n",
    "table_name = f\"soni.default.test_variant_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"üìù Testing table creation: {table_name}\")\n",
    "\n",
    "# Test VARIANT column creation\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INT,\n",
    "        data VARIANT\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "    print(f\"‚úÖ VARIANT table created successfully: {table_name}\")\n",
    "    \n",
    "    # Test VARIANT data insertion\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {table_name} VALUES \n",
    "    (1, parse_json('{{ \"name\": \"test\", \"value\": 123 }}'))\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ VARIANT data inserted successfully\")\n",
    "    \n",
    "    # Test VARIANT data querying\n",
    "    result = spark.sql(f\"\"\"\n",
    "    SELECT id, data:name::STRING as name, data:value::INT as value \n",
    "    FROM {table_name}\n",
    "    \"\"\")\n",
    "    result.show()\n",
    "    print(\"‚úÖ VARIANT data queried successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "variant_iot_sensor_data_demo",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
