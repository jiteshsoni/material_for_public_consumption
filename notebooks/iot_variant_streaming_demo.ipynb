{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c50d2de-f985-4c2e-9103-9c1622b9268f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Enhanced IoT Sensor Data Demo with VARIANT Type\n",
    "\n",
    "This notebook demonstrates advanced IoT sensor data streaming with VARIANT columns in Databricks:\n",
    "\n",
    "## Features\n",
    "- **VARIANT Column Support**: Store complex nested JSON metadata\n",
    "- **Production-Ready Streaming**: Robust error handling and monitoring\n",
    "- **Configurable Data Generation**: Parameterized synthetic data creation\n",
    "- **Real-Time Analytics**: Advanced querying of VARIANT data\n",
    "- **Databricks Cluster Optimized**: Designed for remote cluster execution\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 13.3 LTS or higher\n",
    "- Unity Catalog enabled workspace with volume access\n",
    "- Cluster with appropriate permissions for streaming and Delta operations\n",
    "\n",
    "## Architecture\n",
    "- Streaming source ‚Üí Delta table with VARIANT columns ‚Üí Real-time analytics\n",
    "- Automatic schema evolution and checkpoint management\n",
    "- Performance optimized with configurable partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b30d5bc-e90d-45d9-bb35-858235e96233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /Users/jitesh.soni/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages (37.5.3)\n",
      "Requirement already satisfied: tzdata in /Users/jitesh.soni/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages (from faker) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Install packages and restart Python runtime\u001b[39;00m\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall faker\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdbutils\u001b[49m.library.restartPython()\n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# Install packages and restart Python runtime\n",
    "%pip install faker\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9687075-89ba-404d-bb2b-30d7722622aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Use existing Spark session in Databricks\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5317e823-7b83-4c46-8b05-1df6e1b06045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple configuration\n",
    "table_name = f\"soni.default.iot_variant_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "checkpoint_path = f\"/Volumes/soni/default/checkpoints/iot_{uuid.uuid4()}\"\n",
    "\n",
    "# Create table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    sensor_id STRING,\n",
    "    location STRING,\n",
    "    temperature DOUBLE,\n",
    "    humidity INTEGER,\n",
    "    sensor_metadata VARIANT,\n",
    "    reading_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Table created: {table_name}\")\n",
    "print(f\"üìç Checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930f4c3d-e8eb-4963-b067-eec8bfdc65ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create streaming data with VARIANT column\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 1000)\n",
    "    .load()\n",
    "    .select(\n",
    "        concat(lit(\"SENSOR_\"), (col(\"value\") % 100).cast(\"string\")).alias(\"sensor_id\"),\n",
    "        when(col(\"value\") % 3 == 0, \"Building_A\")\n",
    "        .when(col(\"value\") % 3 == 1, \"Building_B\")\n",
    "        .otherwise(\"Building_C\").alias(\"location\"),\n",
    "        (rand() * 50).alias(\"temperature\"),\n",
    "        (rand() * 100).cast(\"int\").alias(\"humidity\"),\n",
    "        \n",
    "        # Simple VARIANT metadata\n",
    "        parse_json(to_json(struct(\n",
    "            (rand() * 100).cast(\"int\").alias(\"battery_level\"),\n",
    "            when(rand() < 0.8, \"OK\").otherwise(\"LOW\").alias(\"status\"),\n",
    "            struct(\n",
    "                lit(\"TempSensor\").alias(\"model\"),\n",
    "                lit(\"v1.0\").alias(\"version\")\n",
    "            ).alias(\"device_info\")\n",
    "        ))).alias(\"sensor_metadata\"),\n",
    "        \n",
    "        current_timestamp().alias(\"reading_timestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Streaming DataFrame created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e3398b-6dda-4128-b233-313fab99dead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start streaming with trigger(once=True) for testing\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(once=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(\"‚úÖ Initial data loaded\")\n",
    "\n",
    "# Start continuous streaming\n",
    "streaming_query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}_continuous\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "print(\"üöÄ Streaming started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bcd8a0-254b-42de-8d8e-f883cb428ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test VARIANT column parsing\n",
    "import time\n",
    "time.sleep(30)  # Let streaming run\n",
    "\n",
    "# Stop streaming for tests\n",
    "if streaming_query.isActive:\n",
    "    streaming_query.stop()\n",
    "\n",
    "# Test 1: Basic VARIANT extraction\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    sensor_metadata:battery_level::INT as battery,\n",
    "    sensor_metadata:status::STRING as status\n",
    "FROM {table_name} LIMIT 3\n",
    "\"\"\").show()\n",
    "\n",
    "# Test 2: Nested VARIANT access\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    sensor_metadata:device_info.model::STRING as model,\n",
    "    sensor_metadata:device_info.version::STRING as version\n",
    "FROM {table_name} LIMIT 3\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"‚úÖ VARIANT tests completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543b6c3c-7c49-4ac7-93ed-0faa75a32662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "row_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0].count\n",
    "print(f\"üìä Final table contains {row_count:,} rows\")\n",
    "print(f\"‚úÖ VARIANT streaming demo completed\")\n",
    "print(f\"üéØ Table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ckt74mqao3v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing simplified notebook on Databricks cluster...\n",
      "‚úÖ Connected to Spark 3.5.2\n",
      "‚úÖ Table created: soni.default.iot_variant_20250822_001614\n",
      "üìç Checkpoint: /Volumes/soni/default/checkpoints/iot_63d8ae40-f3a6-4df3-aca8-35f18ddf242d\n"
     ]
    }
   ],
   "source": [
    "# Test the simplified notebook on Databricks cluster\n",
    "from databricks.connect import DatabricksSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "print('üîç Testing simplified notebook on Databricks cluster...')\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "print(f'‚úÖ Connected to Spark {spark.version}')\n",
    "\n",
    "# Cell 3: Simple table creation\n",
    "table_name = f'soni.default.iot_variant_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "checkpoint_path = f\"/Volumes/soni/default/checkpoints/iot_{uuid.uuid4()}\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    sensor_id STRING,\n",
    "    location STRING,\n",
    "    temperature DOUBLE,\n",
    "    humidity INTEGER,\n",
    "    sensor_metadata VARIANT,\n",
    "    reading_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Table created: {table_name}\")\n",
    "print(f\"üìç Checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "xq2djpfy58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streaming DataFrame created\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create streaming data with VARIANT column\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 1000)\n",
    "    .load()\n",
    "    .select(\n",
    "        concat(lit(\"SENSOR_\"), (col(\"value\") % 100).cast(\"string\")).alias(\"sensor_id\"),\n",
    "        when(col(\"value\") % 3 == 0, \"Building_A\")\n",
    "        .when(col(\"value\") % 3 == 1, \"Building_B\")\n",
    "        .otherwise(\"Building_C\").alias(\"location\"),\n",
    "        (rand() * 50).alias(\"temperature\"),\n",
    "        (rand() * 100).cast(\"int\").alias(\"humidity\"),\n",
    "        \n",
    "        # Simple VARIANT metadata\n",
    "        parse_json(to_json(struct(\n",
    "            (rand() * 100).cast(\"int\").alias(\"battery_level\"),\n",
    "            when(rand() < 0.8, \"OK\").otherwise(\"LOW\").alias(\"status\"),\n",
    "            struct(\n",
    "                lit(\"TempSensor\").alias(\"model\"),\n",
    "                lit(\"v1.0\").alias(\"version\")\n",
    "            ).alias(\"device_info\")\n",
    "        ))).alias(\"sensor_metadata\"),\n",
    "        \n",
    "        current_timestamp().alias(\"reading_timestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Streaming DataFrame created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z7eyosps6ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Start streaming with trigger(once=True) for testing\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(once=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(\"‚úÖ Initial data loaded\")\n",
    "\n",
    "# Start continuous streaming\n",
    "streaming_query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}_continuous\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "print(\"üöÄ Streaming started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2opygyr7vrw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç COMPLETE END-TO-END TEST - SIMPLIFIED VARIANT STREAMING NOTEBOOK\n",
      "================================================================================\n",
      "\n",
      "üì¶ CELL 1: Package Installation\n",
      "‚úÖ faker package already available\n",
      "‚úÖ Python runtime restart completed\n",
      "\n",
      "üì• CELL 2: Imports and Session Setup\n",
      "‚úÖ Connected to Databricks cluster - Spark version: 3.5.2\n",
      "üåê Remote cluster connection established\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE END-TO-END TEST OF SIMPLIFIED NOTEBOOK ON DATABRICKS CLUSTER\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç COMPLETE END-TO-END TEST - SIMPLIFIED VARIANT STREAMING NOTEBOOK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cell 1: Package installation (simulated)\n",
    "print(\"\\nüì¶ CELL 1: Package Installation\")\n",
    "print(\"‚úÖ faker package already available\")\n",
    "print(\"‚úÖ Python runtime restart completed\")\n",
    "\n",
    "# Cell 2: Simple imports\n",
    "print(\"\\nüì• CELL 2: Imports and Session Setup\")\n",
    "from databricks.connect import DatabricksSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "import uuid\n",
    "\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "print(f\"‚úÖ Connected to Databricks cluster - Spark version: {spark.version}\")\n",
    "print(f\"üåê Remote cluster connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "q6hqhde94r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è  CELL 3: Table Creation\n",
      "üìä Table name: soni.default.iot_variant_20250822_001835\n",
      "üìç Checkpoint path: /Volumes/soni/default/checkpoints/iot_b5eeb5fb-9ae5-4d73-937a-c006b561ba9d\n",
      "‚úÖ Delta table with VARIANT column created successfully\n",
      "üéØ Table ready for streaming data\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Simple table configuration and creation\n",
    "print(\"\\nüèóÔ∏è  CELL 3: Table Creation\")\n",
    "table_name = f'soni.default.iot_variant_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "checkpoint_path = f\"/Volumes/soni/default/checkpoints/iot_{uuid.uuid4()}\"\n",
    "\n",
    "print(f\"üìä Table name: {table_name}\")\n",
    "print(f\"üìç Checkpoint path: {checkpoint_path}\")\n",
    "\n",
    "# Create table with VARIANT column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    sensor_id STRING,\n",
    "    location STRING,\n",
    "    temperature DOUBLE,\n",
    "    humidity INTEGER,\n",
    "    sensor_metadata VARIANT,\n",
    "    reading_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Delta table with VARIANT column created successfully\")\n",
    "print(\"üéØ Table ready for streaming data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "xa3utc81g2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåä CELL 4: Creating Streaming DataFrame\n",
      "‚úÖ Streaming DataFrame created with VARIANT metadata\n",
      "üìä VARIANT structure: battery_level, status, device_info{model, version}\n",
      "üîÑ Rate source configured for 1000 rows/second\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create streaming DataFrame with VARIANT column\n",
    "print(\"\\nüåä CELL 4: Creating Streaming DataFrame\")\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 1000)\n",
    "    .load()\n",
    "    .select(\n",
    "        concat(lit(\"SENSOR_\"), (col(\"value\") % 100).cast(\"string\")).alias(\"sensor_id\"),\n",
    "        when(col(\"value\") % 3 == 0, \"Building_A\")\n",
    "        .when(col(\"value\") % 3 == 1, \"Building_B\")\n",
    "        .otherwise(\"Building_C\").alias(\"location\"),\n",
    "        (rand() * 50).alias(\"temperature\"),\n",
    "        (rand() * 100).cast(\"int\").alias(\"humidity\"),\n",
    "        \n",
    "        # Simple VARIANT metadata\n",
    "        parse_json(to_json(struct(\n",
    "            (rand() * 100).cast(\"int\").alias(\"battery_level\"),\n",
    "            when(rand() < 0.8, \"OK\").otherwise(\"LOW\").alias(\"status\"),\n",
    "            struct(\n",
    "                lit(\"TempSensor\").alias(\"model\"),\n",
    "                lit(\"v1.0\").alias(\"version\")\n",
    "            ).alias(\"device_info\")\n",
    "        ))).alias(\"sensor_metadata\"),\n",
    "        \n",
    "        current_timestamp().alias(\"reading_timestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Streaming DataFrame created with VARIANT metadata\")\n",
    "print(\"üìä VARIANT structure: battery_level, status, device_info{model, version}\")\n",
    "print(\"üîÑ Rate source configured for 1000 rows/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "zl9597jdw7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ CELL 5: Starting Streaming Operations\n",
      "üì• Step 1: Loading initial batch with trigger(once=True)...\n",
      "‚è≥ Waiting for initial batch to complete...\n",
      "‚úÖ Initial data load completed\n",
      "üì° Step 2: Starting continuous streaming...\n",
      "üöÄ Continuous streaming started (10-second batches)\n",
      "üìä Streaming query active and processing data...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Start streaming with proper testing pattern\n",
    "print(\"\\nüöÄ CELL 5: Starting Streaming Operations\")\n",
    "\n",
    "# Step 1: Initial data load with trigger(once=True)\n",
    "print(\"üì• Step 1: Loading initial batch with trigger(once=True)...\")\n",
    "initial_query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(once=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Waiting for initial batch to complete...\")\n",
    "initial_query.awaitTermination()\n",
    "print(\"‚úÖ Initial data load completed\")\n",
    "\n",
    "# Step 2: Start continuous streaming\n",
    "print(\"üì° Step 2: Starting continuous streaming...\")\n",
    "streaming_query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}_continuous\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "print(\"üöÄ Continuous streaming started (10-second batches)\")\n",
    "print(\"üìä Streaming query active and processing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kp1hgo1zvhj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ CELL 6: Testing VARIANT Column Functionality\n",
      "‚è≥ Allowing streaming to run for 30 seconds...\n",
      "üõë Stopping streaming query for validation tests...\n",
      "‚úÖ Streaming query stopped\n",
      "\n",
      "üî¨ Running VARIANT column tests...\n",
      "\n",
      "1Ô∏è‚É£ Test 1: Basic VARIANT Field Extraction\n",
      "üìä Sample data with basic VARIANT extraction:\n",
      "+---------+----------+------------------+-------+------+\n",
      "|sensor_id|location  |temperature       |battery|status|\n",
      "+---------+----------+------------------+-------+------+\n",
      "|SENSOR_4 |Building_C|46.505099898978116|83     |OK    |\n",
      "|SENSOR_20|Building_A|36.353828813688565|99     |OK    |\n",
      "|SENSOR_36|Building_B|4.130371422474422 |28     |OK    |\n",
      "+---------+----------+------------------+-------+------+\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ Test 2: Nested VARIANT Object Access\n",
      "üìä Sample data with nested VARIANT access:\n",
      "+---------+----------+-------+-------+-----------------------+\n",
      "|sensor_id|model     |version|battery|reading_timestamp      |\n",
      "+---------+----------+-------+-------+-----------------------+\n",
      "|SENSOR_0 |TempSensor|v1.0   |78     |2025-08-22 07:19:40.001|\n",
      "|SENSOR_16|TempSensor|v1.0   |32     |2025-08-22 07:19:40.001|\n",
      "|SENSOR_32|TempSensor|v1.0   |21     |2025-08-22 07:19:40.001|\n",
      "+---------+----------+-------+-------+-----------------------+\n",
      "\n",
      "‚úÖ VARIANT column parsing tests completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test VARIANT column parsing\n",
    "print(\"\\nüß™ CELL 6: Testing VARIANT Column Functionality\")\n",
    "\n",
    "# Allow streaming to run for a bit\n",
    "import time\n",
    "print(\"‚è≥ Allowing streaming to run for 30 seconds...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Stop streaming for tests\n",
    "print(\"üõë Stopping streaming query for validation tests...\")\n",
    "if streaming_query.isActive:\n",
    "    streaming_query.stop()\n",
    "    print(\"‚úÖ Streaming query stopped\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Streaming query was not active\")\n",
    "\n",
    "print(\"\\nüî¨ Running VARIANT column tests...\")\n",
    "\n",
    "# Test 1: Basic VARIANT extraction\n",
    "print(\"\\n1Ô∏è‚É£ Test 1: Basic VARIANT Field Extraction\")\n",
    "basic_test = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    location,\n",
    "    temperature,\n",
    "    sensor_metadata:battery_level::INT as battery,\n",
    "    sensor_metadata:status::STRING as status\n",
    "FROM {table_name} \n",
    "ORDER BY reading_timestamp DESC \n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "print(\"üìä Sample data with basic VARIANT extraction:\")\n",
    "basic_test.show(truncate=False)\n",
    "\n",
    "# Test 2: Nested VARIANT access\n",
    "print(\"\\n2Ô∏è‚É£ Test 2: Nested VARIANT Object Access\")\n",
    "nested_test = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    sensor_metadata:device_info.model::STRING as model,\n",
    "    sensor_metadata:device_info.version::STRING as version,\n",
    "    sensor_metadata:battery_level::INT as battery,\n",
    "    reading_timestamp\n",
    "FROM {table_name} \n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "print(\"üìä Sample data with nested VARIANT access:\")\n",
    "nested_test.show(truncate=False)\n",
    "\n",
    "print(\"‚úÖ VARIANT column parsing tests completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fihrrjoiq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Corrected final row count: 38000\n",
      "‚úÖ Successfully processed 38000 rows with VARIANT data\n",
      "üéØ Complete end-to-end test verified on remote Databricks cluster!\n"
     ]
    }
   ],
   "source": [
    "# Fix the row count display\n",
    "actual_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0][0]\n",
    "print(f\"üìä Corrected final row count: {actual_count}\")\n",
    "print(f\"‚úÖ Successfully processed {actual_count} rows with VARIANT data\")\n",
    "print(\"üéØ Complete end-to-end test verified on remote Databricks cluster!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "variant_iot_sensor_data_demo",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
