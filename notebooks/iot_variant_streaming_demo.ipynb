{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c50d2de-f985-4c2e-9103-9c1622b9268f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# IoT Sensor Data Demo with VARIANT Type\n",
    "\n",
    "This notebook demonstrates IoT sensor data streaming with VARIANT columns in Databricks:\n",
    "\n",
    "## Features\n",
    "- **VARIANT Column Support**: Store complex nested JSON metadata\n",
    "- **Realistic Data Generation**: Uses dbldatagen for realistic IoT sensor data\n",
    "- **Streaming Processing**: Real-time data ingestion and processing\n",
    "- **Simple Implementation**: Clean, focused code demonstrating core functionality\n",
    "- **Databricks Cluster Optimized**: Designed for remote cluster execution\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 13.3 LTS or higher\n",
    "- Unity Catalog enabled workspace with volume access\n",
    "- Cluster with appropriate permissions for streaming and Delta operations\n",
    "- dbldatagen library for realistic data generation\n",
    "\n",
    "## Architecture\n",
    "- dbldatagen â†’ Streaming source â†’ Delta table with VARIANT columns â†’ Real-time analytics\n",
    "- Realistic IoT sensor data with weighted distributions and proper data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b30d5bc-e90d-45d9-bb35-858235e96233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install packages and restart Python runtime\n",
    "%pip install dbldatagen jmespath pyparsing\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9687075-89ba-404d-bb2b-30d7722622aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "import dbldatagen as dg\n",
    "\n",
    "# Use existing Spark session in Databricks\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "print(f\"âœ… Spark version: {spark.version}\")\n",
    "print(f\"ðŸ“¦ dbldatagen imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5317e823-7b83-4c46-8b05-1df6e1b06045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple configuration\n",
    "table_name = f\"soni.default.iot_variant_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "checkpoint_path = f\"/Volumes/soni/default/checkpoints/iot_{uuid.uuid4()}\"\n",
    "\n",
    "# Create table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    sensor_id STRING,\n",
    "    location STRING,\n",
    "    temperature DOUBLE,\n",
    "    humidity INTEGER,\n",
    "    sensor_metadata VARIANT,\n",
    "    reading_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Table created: {table_name}\")\n",
    "print(f\"ðŸ“ Checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930f4c3d-e8eb-4963-b067-eec8bfdc65ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create realistic IoT streaming data with dbldatagen\n",
    "print(\"ðŸ”§ Creating realistic IoT streaming data with dbldatagen...\")\n",
    "\n",
    "# Define IoT sensor schema\n",
    "iot_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"temperature\", DoubleType(), False),\n",
    "    StructField(\"humidity\", IntegerType(), False),\n",
    "    StructField(\"battery_level\", IntegerType(), False),\n",
    "    StructField(\"signal_strength\", IntegerType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"firmware_version\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Create dbldatagen specification\n",
    "dataspec = (\n",
    "    dg.DataGenerator(spark, name=\"iot_sensors\", partitions=8)\n",
    "    .withSchema(iot_schema)\n",
    "    .withColumnSpec(\"sensor_id\", minValue=1, maxValue=100, prefix=\"SENSOR_\", random=True)\n",
    "    .withColumnSpec(\"location\", values=[\"Building_A\", \"Building_B\", \"Building_C\", \"Warehouse\", \"DataCenter\"], \n",
    "                   weights=[0.25, 0.25, 0.25, 0.15, 0.1], random=True)\n",
    "    .withColumnSpec(\"temperature\", minValue=-10.0, maxValue=50.0, random=True)\n",
    "    .withColumnSpec(\"humidity\", minValue=30, maxValue=90, random=True)\n",
    "    .withColumnSpec(\"battery_level\", minValue=1, maxValue=100, random=True)\n",
    "    .withColumnSpec(\"signal_strength\", minValue=-100, maxValue=-20, random=True)\n",
    "    .withColumnSpec(\"status\", values=[\"OK\", \"SENSOR_FAIL\", \"BATTERY_LOW\", \"COMM_LOSS\"],\n",
    "                   weights=[0.8, 0.05, 0.1, 0.05], random=True)\n",
    "    .withColumnSpec(\"firmware_version\", values=[\"v1.0\", \"v2.0\", \"v2.1\"], \n",
    "                   weights=[0.2, 0.3, 0.5], random=True)\n",
    ")\n",
    "\n",
    "# Build streaming DataFrame with VARIANT metadata\n",
    "streaming_df = (\n",
    "    dataspec.build(\n",
    "        withStreaming=True,\n",
    "        options={\n",
    "            'rowsPerSecond': 1000,\n",
    "            'numPartitions': 8\n",
    "        }\n",
    "    )\n",
    "    .withColumn(\"reading_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Create complex VARIANT metadata from the generated columns\n",
    "    .withColumn(\"sensor_metadata\", \n",
    "        parse_json(to_json(struct(\n",
    "            col(\"battery_level\").alias(\"battery_level\"),\n",
    "            col(\"signal_strength\").alias(\"signal_strength\"),\n",
    "            col(\"status\").alias(\"status\"),\n",
    "            col(\"firmware_version\").alias(\"firmware_version\"),\n",
    "            current_timestamp().alias(\"last_maintenance\"),\n",
    "            \n",
    "            # Device information\n",
    "            struct(\n",
    "                lit(\"Acme Corp\").alias(\"manufacturer\"),\n",
    "                lit(\"TempSense Pro\").alias(\"model\"),\n",
    "                lit(\"2023\").alias(\"year\"),\n",
    "                concat(lit(\"TS-\"), col(\"sensor_id\")).alias(\"part_number\")\n",
    "            ).alias(\"device_info\"),\n",
    "            \n",
    "            # Network connectivity  \n",
    "            struct(\n",
    "                lit(\"WiFi\").alias(\"connection_type\"),\n",
    "                (col(\"sensor_id\").cast(\"int\") % 10 + 1).alias(\"network_id\"),\n",
    "                concat(lit(\"192.168.1.\"), (col(\"sensor_id\").cast(\"int\") % 254 + 1).cast(\"string\")).alias(\"ip_address\")\n",
    "            ).alias(\"network\"),\n",
    "            \n",
    "            # Environmental conditions\n",
    "            struct(\n",
    "                col(\"temperature\").alias(\"ambient_temp\"),\n",
    "                col(\"humidity\").alias(\"ambient_humidity\"),\n",
    "                (rand() * 200 + 800).alias(\"pressure_hpa\")\n",
    "            ).alias(\"environment\")\n",
    "        )))\n",
    "    )\n",
    "    \n",
    "    # Keep only the final columns we want\n",
    "    .select(\"sensor_id\", \"location\", \"temperature\", \"humidity\", \"sensor_metadata\", \"reading_timestamp\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Realistic IoT streaming DataFrame created with dbldatagen\")\n",
    "print(\"ðŸ“Š VARIANT metadata includes: battery, signal, status, device_info, network, environment\")\n",
    "print(\"ðŸŽ¯ Data generation: 1000 rows/second with realistic distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e3398b-6dda-4128-b233-313fab99dead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start streaming with trigger(once=True) for testing\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(once=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(\"âœ… Initial data loaded\")\n",
    "\n",
    "# Start continuous streaming\n",
    "streaming_query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}_continuous\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Streaming started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bcd8a0-254b-42de-8d8e-f883cb428ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test VARIANT column parsing\n",
    "import time\n",
    "time.sleep(30)  # Let streaming run\n",
    "\n",
    "# Stop streaming for tests\n",
    "if streaming_query.isActive:\n",
    "    streaming_query.stop()\n",
    "\n",
    "# Test 1: Basic VARIANT extraction\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    sensor_metadata:battery_level::INT as battery,\n",
    "    sensor_metadata:status::STRING as status\n",
    "FROM {table_name} LIMIT 3\n",
    "\"\"\").show()\n",
    "\n",
    "# Test 2: Nested VARIANT access\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    sensor_metadata:device_info.model::STRING as model,\n",
    "    sensor_metadata:device_info.version::STRING as version\n",
    "FROM {table_name} LIMIT 3\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"âœ… VARIANT tests completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543b6c3c-7c49-4ac7-93ed-0faa75a32662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "row_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0].count\n",
    "print(f\"ðŸ“Š Final table contains {row_count:,} rows\")\n",
    "print(f\"âœ… VARIANT streaming demo completed\")\n",
    "print(f\"ðŸŽ¯ Table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uia54bwyrbh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Installing missing jmespath dependency...\n",
      "Collecting jmespath\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: jmespath\n",
      "Successfully installed jmespath-1.0.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyparsing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjmespath\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Now test dbldatagen\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdbldatagen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdg\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… dbldatagen imported successfully after installing jmespath\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Test with Databricks Connect\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/dbldatagen/__init__.py:26\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright (C) 2019 Databricks, Inc.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03mThis module defines the package contents for the test data generator library\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33;03mMost of the other classes are used for internal purposes only\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataGenerator\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatagen_constants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_RANDOM_SEED, RANDOM_SEED_RANDOM, RANDOM_SEED_FIXED, \\\n\u001b[32m     28\u001b[39m                                RANDOM_SEED_HASH_FIELD_NAME, MIN_PYTHON_VERSION, MIN_SPARK_VERSION, \\\n\u001b[32m     29\u001b[39m                                INFER_DATATYPE, SPARK_DEFAULT_PARALLELISM\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure, topologicalSort, mkBoundsList, coalesce_values, \\\n\u001b[32m     31\u001b[39m     deprecated, parse_time_interval, DataGenError, split_list_matching_condition, strip_margins, \\\n\u001b[32m     32\u001b[39m     json_value_from_path, system_time_millis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/dbldatagen/data_generator.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LongType, IntegerType, StringType, StructType, StructField, DataType\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_spark_version\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn_generation_spec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnGenerationSpec\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstraints\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstraint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Constraint\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstraints\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql_expr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SqlExpr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/dbldatagen/column_generation_spec.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_generators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TemplateGenerator\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure, coalesce_values\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchemaParser\n\u001b[32m     32\u001b[39m HASH_COMPUTE_METHOD = \u001b[33m\"\u001b[39m\u001b[33mhash\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m VALUES_COMPUTE_METHOD = \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor_base/dlt_auto_scaling/material_for_public_consumption/.venv/lib/python3.12/site-packages/dbldatagen/schema_parser.py:10\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mThis file defines the `SchemaParser` class\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyparsing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LongType, FloatType, IntegerType, StringType, DoubleType, BooleanType, ShortType, \\\n\u001b[32m     12\u001b[39m     TimestampType, DateType, DecimalType, ByteType, BinaryType, StructField, StructType, MapType, ArrayType\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSchemaParser\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyparsing'"
     ]
    }
   ],
   "source": [
    "# Install missing dependency\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ðŸ”§ Installing missing jmespath dependency...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"jmespath\"])\n",
    "\n",
    "# Now test dbldatagen\n",
    "import dbldatagen as dg\n",
    "print(\"âœ… dbldatagen imported successfully after installing jmespath\")\n",
    "\n",
    "# Test with Databricks Connect\n",
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "# Test basic dbldatagen functionality\n",
    "test_spec = dg.DataGenerator(spark, name=\"test\", partitions=1)\n",
    "print(\"âœ… dbldatagen DataGenerator works on Databricks cluster\")\n",
    "\n",
    "print(\"ðŸŽ¯ Notebook should work now - dbldatagen is properly functional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pa27vvxxpsd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies and test\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install all missing dependencies\n",
    "dependencies = [\"jmespath\", \"pyparsing\"]\n",
    "for dep in dependencies:\n",
    "    print(f\"Installing {dep}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep])\n",
    "\n",
    "# Test dbldatagen import\n",
    "import dbldatagen as dg\n",
    "print(\"âœ… dbldatagen imported successfully!\")\n",
    "\n",
    "# Test with Databricks Connect\n",
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "# Test basic functionality\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "test_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False)\n",
    "])\n",
    "\n",
    "test_spec = (\n",
    "    dg.DataGenerator(spark, name=\"test\", partitions=1)\n",
    "    .withSchema(test_schema)\n",
    "    .withColumnSpec(\"id\", minValue=1, maxValue=100)\n",
    "    .withColumnSpec(\"name\", values=[\"Alice\", \"Bob\"], random=True)\n",
    ")\n",
    "\n",
    "print(\"âœ… dbldatagen DataGenerator configuration successful\")\n",
    "\n",
    "# Test streaming build\n",
    "test_df = test_spec.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "print(\"âœ… dbldatagen streaming build successful\")\n",
    "\n",
    "print(\"ðŸŽ¯ All dependencies work - notebook is ready for testing!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "variant_iot_sensor_data_demo",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
