{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IoT Sensor Data Demo with VARIANT Type\n",
        "\n",
        "This notebook demonstrates working with VARIANT columns in Databricks, specifically:\n",
        "- Creating tables with VARIANT columns\n",
        "- Generating synthetic IoT sensor data\n",
        "- Storing nested metadata as VARIANT\n",
        "- Querying and analyzing VARIANT data\n",
        "\n",
        "## Prerequisites\n",
        "- Databricks Connect configured locally\n",
        "- Environment variables set: DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAGIC %pip install dbldatagen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”— Initializing Databricks Connect...\n",
            "âŒ Connection failed: No module named 'databricks'\n",
            "ðŸ’¡ Try restarting the kernel and running again\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'databricks'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”— Initializing Databricks Connect...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatabricks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatabricksSession\n\u001b[32m      5\u001b[39m     spark = DatabricksSession.builder.getOrCreate()\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Connected to Databricks cluster via Databricks Connect\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'databricks'"
          ]
        }
      ],
      "source": [
        "# Initialize Databricks Connect\n",
        "print(\"ðŸ”— Initializing Databricks Connect...\")\n",
        "try:\n",
        "    from databricks.connect import DatabricksSession\n",
        "    spark = DatabricksSession.builder.getOrCreate()\n",
        "    print(\"âœ… Connected to Databricks cluster via Databricks Connect\")\n",
        "    \n",
        "    # Verify connection\n",
        "    print(f\"âœ… Spark version: {spark.version}\")\n",
        "    test_count = spark.range(3).count()\n",
        "    print(f\"âœ… Connected to remote cluster: {test_count} test rows\")\n",
        "    \n",
        "    # Check Databricks Runtime version\n",
        "    try:\n",
        "        version_info = spark.sql(\"SELECT current_version() as version\").collect()[0]\n",
        "        print(f\"ðŸ¢ Databricks Runtime: {version_info.version}\")\n",
        "        print(\"âœ… CONFIRMED: Running on DATABRICKS CLUSTER\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Could not get runtime version: {str(e)[:50]}...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Connection failed: {e}\")\n",
        "    print(\"ðŸ’¡ Try restarting the kernel and running again\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
        "from pyspark.sql.functions import expr, when, col, concat, lit, to_json, struct, current_timestamp, make_interval, parse_json, floor, rand\n",
        "import dbldatagen as dg\n",
        "import uuid\n",
        "\n",
        "print(\"âœ… Required imports completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for high-throughput streaming\n",
        "PARTITIONS = 12  # Increased for better streaming performance\n",
        "ROWS_PER_SECOND = 100000  # 100K rows/sec (adjust based on your cluster capacity)\n",
        "\n",
        "# Checkpoint location for streaming\n",
        "CHECKPOINT_BASE = \"s3://test-external-volume-bucket-2/test-folder\"\n",
        "CHECKPOINT_PATH = f\"{CHECKPOINT_BASE}/iot_variant_stream_checkpoint-{uuid.uuid4()}\"\n",
        "\n",
        "# Base schema for generating data\n",
        "base_schema = StructType([\n",
        "    StructField(\"sensor_id\", StringType(), False),\n",
        "    StructField(\"location\", StringType(), False),\n",
        "    StructField(\"temperature\", DoubleType(), False),\n",
        "    StructField(\"humidity\", IntegerType(), False),\n",
        "    StructField(\"pressure\", DoubleType(), False),\n",
        "    StructField(\"battery_level\", IntegerType(), False),\n",
        "    StructField(\"signal_strength\", IntegerType(), False),\n",
        "    StructField(\"fault_code\", StringType(), False)\n",
        "])\n",
        "\n",
        "print(\"ðŸ”§ Streaming Configuration:\")\n",
        "print(f\"   â€¢ Partitions: {PARTITIONS}\")\n",
        "print(f\"   â€¢ Rows/second: {ROWS_PER_SECOND:,}\")\n",
        "print(f\"   â€¢ Checkpoint: {CHECKPOINT_PATH}\")\n",
        "print(\"âœ… Schema and configuration set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a table with VARIANT column for metadata and flat columns for readings\n",
        "print(\"ðŸ—ï¸ Creating table with VARIANT and flat columns...\")\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS soni.default.iot_sensor_variant_data (\n",
        "    sensor_id STRING,\n",
        "    location STRING,\n",
        "    sensor_metadata VARIANT,\n",
        "    temperature DOUBLE,\n",
        "    humidity INTEGER,\n",
        "    pressure DOUBLE,\n",
        "    reading_timestamp TIMESTAMP,\n",
        "    temp_status STRING,\n",
        "    sample_count INTEGER,\n",
        "    event_timestamp TIMESTAMP\n",
        ")\n",
        "\"\"\")\n",
        "print(\"âœ… Table created successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build streaming DataFrame with optimizations\n",
        "print(\"ðŸ”„ Setting up streaming data generator...\")\n",
        "streaming_df = (\n",
        "    dataspec.build(\n",
        "        withStreaming=True,\n",
        "        options={\n",
        "            'rowsPerSecond': ROWS_PER_SECOND,\n",
        "            'numPartitions': PARTITIONS,\n",
        "        }\n",
        "    )\n",
        "    # Create sensor_metadata VARIANT with nested integer, string, and timestamp\n",
        "    .withColumn(\"sensor_metadata\", \n",
        "                to_json(struct(\n",
        "                    col(\"battery_level\").alias(\"battery_level\"),  # INTEGER\n",
        "                    col(\"signal_strength\").alias(\"signal_strength\"),  # INTEGER\n",
        "                    col(\"fault_code\").alias(\"status\"),  # STRING\n",
        "                    current_timestamp().alias(\"last_maintenance\"),  # TIMESTAMP\n",
        "                    current_timestamp().alias(\"installation_date\"),  # TIMESTAMP\n",
        "                    lit(\"v2.1.4\").alias(\"firmware_version\"),  # STRING\n",
        "                    (floor(rand() * 1000) + 1).alias(\"calibration_count\")  # INTEGER\n",
        "                )))\n",
        "    # Create flat sensor reading columns with optimized expressions\n",
        "    .withColumn(\"reading_timestamp\", current_timestamp())\n",
        "    .withColumn(\"temp_status\", when(col(\"temperature\") > 35, \"HIGH_TEMP\")\n",
        "                              .when(col(\"temperature\") < 5, \"LOW_TEMP\")\n",
        "                              .otherwise(\"NORMAL\"))\n",
        "    .withColumn(\"sample_count\", floor(rand() * 100) + 1)\n",
        "    .withColumn(\"event_timestamp\", current_timestamp())\n",
        "    # Deterministic tracker for compression (1000x reduction)\n",
        "    .withColumn(\"tracker_row\", expr(\"abs(hash(sensor_id)) % 1000 + 1\"))\n",
        "    # Parse JSON to VARIANT efficiently\n",
        "    .withColumn(\"sensor_metadata_variant\", parse_json(col(\"sensor_metadata\")))\n",
        "    .drop(\"sensor_metadata\")\n",
        "    .withColumnRenamed(\"sensor_metadata_variant\", \"sensor_metadata\")\n",
        ")\n",
        "\n",
        "print(\"âœ… Streaming DataFrame configured with optimizations:\")\n",
        "print(\"   â€¢ Streaming mode enabled\")\n",
        "print(\"   â€¢ Deterministic tracking for compression\")\n",
        "print(\"   â€¢ Efficient timestamp generation\")\n",
        "print(\"   â€¢ Optimized column expressions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the streaming data to Delta table with VARIANT columns\n",
        "print(\"ðŸš€ Starting streaming write operation...\")\n",
        "\n",
        "(\n",
        "    streaming_df.writeStream\n",
        "        .queryName(\"iot_variant_sensor_stream\")\n",
        "        .outputMode(\"append\")\n",
        "        .format(\"delta\")\n",
        "        .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
        "        .toTable(\"soni.default.iot_sensor_variant_data\")\n",
        ")\n",
        "\n",
        "print(\"âœ… Streaming write started to soni.default.iot_sensor_variant_data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the streaming data to Delta table with VARIANT columns\n",
        "print(\"ðŸš€ Starting streaming write operation...\")\n",
        "\n",
        "streaming_query = (\n",
        "    streaming_df.writeStream\n",
        "        .queryName(\"iot_variant_sensor_stream\")\n",
        "        .outputMode(\"append\")\n",
        "        .format(\"delta\")\n",
        "        .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
        "        .option(\"mergeSchema\", \"true\")  # Handle schema evolution for VARIANT\n",
        "        .toTable(\"soni.default.iot_sensor_variant_data\")\n",
        ")\n",
        "\n",
        "print(\"âœ… Streaming write configured:\")\n",
        "print(f\"   â€¢ Query Name: {streaming_query.name}\")\n",
        "print(f\"   â€¢ Status: {streaming_query.status}\")\n",
        "print(\"   â€¢ Mode: append\")\n",
        "print(\"   â€¢ Format: delta\")\n",
        "print(\"   â€¢ Schema evolution: enabled\")\n",
        "\n",
        "# Optional: Show current streaming status\n",
        "print(\"\\nðŸ“Š Active Streams:\")\n",
        "for stream in spark.streams.active:\n",
        "    print(f\"   â€¢ {stream.name}: {stream.status['message']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor streaming metrics\n",
        "def monitor_streaming_metrics():\n",
        "    \"\"\"Display current streaming metrics\"\"\"\n",
        "    if not spark.streams.active:\n",
        "        print(\"âŒ No active streaming queries\")\n",
        "        return\n",
        "    \n",
        "    print(\"ðŸ“Š Streaming Metrics:\")\n",
        "    for query in spark.streams.active:\n",
        "        print(f\"\\nQuery: {query.name}\")\n",
        "        print(f\"Status: {query.status['message']}\")\n",
        "        \n",
        "        # Get recent progress metrics\n",
        "        recent_progress = query.recentProgress\n",
        "        if recent_progress:\n",
        "            latest = recent_progress[-1]\n",
        "            print(\"\\nLatest Metrics:\")\n",
        "            print(f\"â€¢ Input rate: {latest.get('inputRowsPerSecond', 0):.0f} rows/second\")\n",
        "            print(f\"â€¢ Processing rate: {latest.get('processedRowsPerSecond', 0):.0f} rows/second\")\n",
        "            print(f\"â€¢ Batch duration: {latest.get('batchDuration', 0):.2f} ms\")\n",
        "            \n",
        "            # Memory metrics\n",
        "            mem_used = latest.get('memoryUsedBytes', 0) / 1024 / 1024  # Convert to MB\n",
        "            print(f\"â€¢ Memory used: {mem_used:.2f} MB\")\n",
        "            \n",
        "            # State metrics if available\n",
        "            state_operators = latest.get('stateOperators', [])\n",
        "            if state_operators:\n",
        "                print(\"\\nState Store Metrics:\")\n",
        "                for op in state_operators:\n",
        "                    print(f\"â€¢ Rows: {op.get('numRowsTotal', 0):,}\")\n",
        "                    print(f\"â€¢ Memory: {op.get('memoryUsedBytes', 0) / 1024 / 1024:.2f} MB\")\n",
        "        else:\n",
        "            print(\"No metrics available yet\")\n",
        "\n",
        "# Run initial monitoring\n",
        "monitor_streaming_metrics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming control functions\n",
        "def stop_streaming():\n",
        "    \"\"\"Stop all streaming queries\"\"\"\n",
        "    print(\"ðŸ›‘ Stopping all streaming queries...\")\n",
        "    for query in spark.streams.active:\n",
        "        query.stop()\n",
        "    print(\"âœ… All streaming queries stopped\")\n",
        "\n",
        "def cleanup_streaming():\n",
        "    \"\"\"Stop streams and clean up resources\"\"\"\n",
        "    print(\"ðŸ§¹ Cleaning up streaming resources...\")\n",
        "    \n",
        "    # Stop all streams\n",
        "    stop_streaming()\n",
        "    \n",
        "    # Clean up checkpoint location\n",
        "    print(\"Removing checkpoint directory...\")\n",
        "    spark.sql(f\"DELETE FROM soni.default.iot_sensor_variant_data WHERE 1=1\")\n",
        "    print(\"âœ… Cleanup completed\")\n",
        "\n",
        "# Example usage:\n",
        "print(\"Available commands:\")\n",
        "print(\"â€¢ monitor_streaming_metrics() - Show current metrics\")\n",
        "print(\"â€¢ stop_streaming() - Stop all streams\")\n",
        "print(\"â€¢ cleanup_streaming() - Stop streams and clean up\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for data generation\n",
        "PARTITIONS = 8\n",
        "ROWS_PER_SECOND = 10000\n",
        "\n",
        "# Base schema for generating data\n",
        "base_schema = StructType([\n",
        "    StructField(\"sensor_id\", StringType(), False),\n",
        "    StructField(\"location\", StringType(), False),\n",
        "    StructField(\"temperature\", DoubleType(), False),\n",
        "    StructField(\"humidity\", IntegerType(), False),\n",
        "    StructField(\"pressure\", DoubleType(), False),\n",
        "    StructField(\"battery_level\", IntegerType(), False),\n",
        "    StructField(\"signal_strength\", IntegerType(), False),\n",
        "    StructField(\"fault_code\", StringType(), False)\n",
        "])\n",
        "\n",
        "print(\"âœ… Schema and configuration set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data generator for base IoT data\n",
        "print(\"ðŸ”„ Setting up data generator...\")\n",
        "dataspec = (\n",
        "    dg.DataGenerator(spark, name=\"iot_variant_data\", partitions=PARTITIONS)\n",
        "    .withSchema(base_schema)\n",
        "    .withColumnSpec(\"sensor_id\", minValue=1000000, maxValue=4000000, prefix=\"SENSOR_\", random=True)\n",
        "    .withColumnSpec(\"location\", \n",
        "                   values=[\"Building_A_Floor_1\", \"Building_A_Floor_2\", \"Building_B_Floor_1\", \"Building_C_Roof\"],\n",
        "                   weights=[0.3, 0.3, 0.2, 0.2], random=True)\n",
        "    .withColumnSpec(\"temperature\", minValue=-10.0, maxValue=40.0, random=True)\n",
        "    .withColumnSpec(\"humidity\", minValue=20, maxValue=90, random=True)\n",
        "    .withColumnSpec(\"pressure\", minValue=980.0, maxValue=1020.0, random=True)\n",
        "    .withColumnSpec(\"battery_level\", minValue=0, maxValue=100, random=True)\n",
        "    .withColumnSpec(\"signal_strength\", minValue=-100, maxValue=0, random=True)\n",
        "    .withColumnSpec(\"fault_code\", \n",
        "                   values=[\"OK\",\"E01_sensor_fail\",\"E02_battery_low\",\"E03_comm_loss\"],\n",
        "                   weights=[0.85,0.05,0.05,0.05], random=True)\n",
        ")\n",
        "print(\"âœ… Data generator configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build batch DataFrame with VARIANT metadata and flat readings for demonstration\n",
        "print(\"ðŸ—ï¸ Building DataFrame with VARIANT metadata...\")\n",
        "batch_df = (\n",
        "    dataspec.build(\n",
        "        withStreaming=False,\n",
        "        options={\n",
        "            'rowsPerSecond': ROWS_PER_SECOND,\n",
        "            'numPartitions': PARTITIONS,\n",
        "        }\n",
        "    )\n",
        "    # Create sensor_metadata VARIANT with nested integer, string, and timestamp\n",
        "    .withColumn(\"sensor_metadata\", \n",
        "                to_json(struct(\n",
        "                    col(\"battery_level\").alias(\"battery_level\"),  # INTEGER\n",
        "                    col(\"signal_strength\").alias(\"signal_strength\"),  # INTEGER\n",
        "                    col(\"fault_code\").alias(\"status\"),  # STRING\n",
        "                    current_timestamp().alias(\"last_maintenance\"),  # TIMESTAMP\n",
        "                    current_timestamp().alias(\"installation_date\"),  # TIMESTAMP\n",
        "                    lit(\"v2.1.4\").alias(\"firmware_version\"),  # STRING\n",
        "                    (floor(rand() * 1000) + 1).alias(\"calibration_count\")  # INTEGER\n",
        "                )))\n",
        "    # Create flat sensor reading columns\n",
        "    .withColumn(\"reading_timestamp\", current_timestamp())\n",
        "    .withColumn(\"temp_status\", when(col(\"temperature\") > 35, \"HIGH_TEMP\").when(col(\"temperature\") < 5, \"LOW_TEMP\").otherwise(\"NORMAL\"))\n",
        "    .withColumn(\"sample_count\", floor(rand() * 100) + 1)\n",
        "    .withColumn(\"event_timestamp\", current_timestamp())\n",
        "    # Parse JSON string to VARIANT type for metadata only\n",
        "    .withColumn(\"sensor_metadata_variant\", parse_json(col(\"sensor_metadata\")))\n",
        "    .drop(\"sensor_metadata\")  # Drop the JSON string column\n",
        "    .withColumnRenamed(\"sensor_metadata_variant\", \"sensor_metadata\")\n",
        ")\n",
        "print(\"âœ… DataFrame built successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample data to see the VARIANT structure\n",
        "print(\"=== Sample Data Preview ===\")\n",
        "batch_df.limit(5).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the batch data to the table with VARIANT columns\n",
        "print(\"=== Writing data to table ===\")\n",
        "batch_df.write.mode(\"overwrite\").saveAsTable(\"soni.default.iot_sensor_variant_data\")\n",
        "print(\"âœ… Data written successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query examples to demonstrate VARIANT column usage\n",
        "\n",
        "# 1. Query sensor metadata from VARIANT column\n",
        "print(\"=== Sensor Metadata Query ===\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "    sensor_id,\n",
        "    location,\n",
        "    sensor_metadata:battery_level as battery_level,\n",
        "    sensor_metadata:status as sensor_status,\n",
        "    sensor_metadata:firmware_version as firmware,\n",
        "    sensor_metadata:calibration_count as calibrations,\n",
        "    sensor_metadata:last_maintenance as last_maintenance\n",
        "FROM soni.default.iot_sensor_variant_data\n",
        "WHERE sensor_metadata:battery_level::INT < 30\n",
        "LIMIT 10\n",
        "\"\"\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Query flat sensor readings columns\n",
        "print(\"=== Sensor Readings Query ===\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "    sensor_id,\n",
        "    location,\n",
        "    temperature as current_temp,\n",
        "    humidity as current_humidity,\n",
        "    pressure as current_pressure,\n",
        "    temp_status as temperature_status,\n",
        "    reading_timestamp as reading_time,\n",
        "    sample_count as samples\n",
        "FROM soni.default.iot_sensor_variant_data\n",
        "WHERE temperature > 30\n",
        "LIMIT 10\n",
        "\"\"\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Complex query combining VARIANT metadata with flat readings\n",
        "print(\"=== Complex VARIANT Query ===\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "    sensor_id,\n",
        "    location,\n",
        "    sensor_metadata:battery_level::INT as battery,\n",
        "    sensor_metadata:status as status,\n",
        "    temperature as temp,\n",
        "    temp_status as temp_status,\n",
        "    reading_timestamp as reading_time,\n",
        "    sensor_metadata:last_maintenance as last_maintenance,\n",
        "    CASE \n",
        "        WHEN sensor_metadata:battery_level::INT < 20 THEN 'CRITICAL_BATTERY'\n",
        "        WHEN temperature > 35 THEN 'HIGH_TEMP_ALERT'\n",
        "        WHEN sensor_metadata:status::STRING != 'OK' THEN 'SENSOR_FAULT'\n",
        "        ELSE 'NORMAL'\n",
        "    END as alert_level\n",
        "FROM soni.default.iot_sensor_variant_data\n",
        "WHERE sensor_metadata:battery_level::INT < 30 OR temperature > 30\n",
        "ORDER BY reading_timestamp DESC\n",
        "LIMIT 15\n",
        "\"\"\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Aggregation query using VARIANT metadata and flat readings\n",
        "print(\"=== VARIANT Aggregation Query ===\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "    location,\n",
        "    COUNT(*) as total_sensors,\n",
        "    AVG(temperature) as avg_temperature,\n",
        "    AVG(humidity) as avg_humidity,\n",
        "    AVG(sensor_metadata:battery_level::INT) as avg_battery,\n",
        "    COUNT(CASE WHEN sensor_metadata:status::STRING != 'OK' THEN 1 END) as faulty_sensors,\n",
        "    COUNT(CASE WHEN temp_status = 'HIGH_TEMP' THEN 1 END) as high_temp_alerts\n",
        "FROM soni.default.iot_sensor_variant_data\n",
        "GROUP BY location\n",
        "ORDER BY avg_battery ASC\n",
        "\"\"\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Time-based analysis using flat timestamp fields\n",
        "print(\"=== Time-based Analysis ===\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "    DATE(reading_timestamp) as reading_date,\n",
        "    HOUR(reading_timestamp) as reading_hour,\n",
        "    COUNT(*) as readings_count,\n",
        "    AVG(temperature) as avg_temp,\n",
        "    AVG(humidity) as avg_humidity,\n",
        "    COUNT(CASE WHEN temp_status = 'HIGH_TEMP' THEN 1 END) as high_temp_count\n",
        "FROM soni.default.iot_sensor_variant_data\n",
        "WHERE reading_timestamp >= current_timestamp() - INTERVAL 1 DAY\n",
        "GROUP BY DATE(reading_timestamp), HOUR(reading_timestamp)\n",
        "ORDER BY reading_date DESC, reading_hour DESC\n",
        "LIMIT 24\n",
        "\"\"\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Test VARIANT functions\n",
        "print(\"=== VARIANT Functions Test ===\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "    sensor_id,\n",
        "    schema_of_variant(sensor_metadata) as metadata_schema,\n",
        "    to_json(sensor_metadata) as metadata_json,\n",
        "    is_variant_null(sensor_metadata) as is_null\n",
        "FROM soni.default.iot_sensor_variant_data\n",
        "LIMIT 3\n",
        "\"\"\").show(truncate=False)\n",
        "\n",
        "print(\"\\nðŸŽ‰ VARIANT IoT Sensor Demo Completed Successfully!\")\n",
        "print(\"ðŸ“š Key Learnings Applied:\")\n",
        "print(\"1. âœ… Used ':' operator for VARIANT field access\")\n",
        "print(\"2. âœ… Used '::' operator for casting VARIANT fields to specific types\")\n",
        "print(\"3. âœ… Combined VARIANT metadata with flat sensor readings\")\n",
        "print(\"4. âœ… Used VARIANT functions like schema_of_variant() and to_json()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup (Optional)\n",
        "\n",
        "Uncomment and run the following cell to clean up the demo resources:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up example\n",
        "# spark.sql(\"DROP TABLE IF EXISTS soni.default.iot_sensor_variant_data\")\n",
        "# print(\"âœ… Cleanup completed\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
