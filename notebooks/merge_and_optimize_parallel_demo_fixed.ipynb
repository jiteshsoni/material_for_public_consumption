{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE and OPTIMIZE Parallel Demo with Databricks Connect\n",
    "\n",
    "This notebook demonstrates that MERGE and OPTIMIZE can run in parallel with row-level concurrency, deletion vectors, and liquid clustering in Databricks.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Connect configured locally\n",
    "- Environment variables set: DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID\n",
    "\n",
    "## Overview\n",
    "\n",
    "The demo creates a Delta table with the following features:\n",
    "- **Row-level concurrency**: Multiple writers can modify different rows simultaneously\n",
    "- **Deletion vectors**: Efficient deletion without rewriting entire files\n",
    "- **Liquid clustering**: Automatic clustering on the merge column for optimal performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f69ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python executable: /Users/jitesh.soni/Documents/Cursor_base/merge_and_optimize_in_parallel_august_19/material_for_public_consumption/.venv/bin/python\n",
      "üêç Python version: 3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "üîó Initializing Databricks Connect...\n",
      "‚úÖ Connected to Databricks cluster via Databricks Connect\n",
      "‚úÖ Spark version: 3.5.2\n",
      "‚úÖ Connected to remote cluster: 3 test rows\n",
      "üè¢ Databricks Runtime: Row(dbr_version='16.4.x-aarch64-photon-scala2.13', dbsql_version=None, u_build_hash='453f7c54a598d89126fa96071946f7c4dea3776f', r_build_hash='b389c7729467c0dd76fe314e6137ee694bb5d4cc')\n",
      "‚úÖ CONFIRMED: Running on DATABRICKS CLUSTER\n",
      "‚úÖ All libraries imported and ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Connection & Setup\n",
    "# üö® IMPORTANT: If you get version errors, RESTART THE KERNEL first!\n",
    "\n",
    "import sys\n",
    "print(f\"üêç Python executable: {sys.executable}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "\n",
    "# Initialize Databricks Connect\n",
    "print(\"üîó Initializing Databricks Connect...\")\n",
    "try:\n",
    "    from databricks.connect import DatabricksSession\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    print(\"‚úÖ Connected to Databricks cluster via Databricks Connect\")\n",
    "    \n",
    "    # Verify connection\n",
    "    print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "    test_count = spark.range(3).count()\n",
    "    print(f\"‚úÖ Connected to remote cluster: {test_count} test rows\")\n",
    "    \n",
    "    # Check Databricks Runtime version\n",
    "    try:\n",
    "        version_info = spark.sql(\"SELECT current_version() as version\").collect()[0]\n",
    "        print(f\"üè¢ Databricks Runtime: {version_info.version}\")\n",
    "        print(\"‚úÖ CONFIRMED: Running on DATABRICKS CLUSTER\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not get runtime version: {str(e)[:50]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"üí° Try restarting the kernel and running again\")\n",
    "    raise\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from faker import Faker\n",
    "from faker_vehicle import VehicleProvider\n",
    "import uuid\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All libraries imported and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0302837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Verifying required packages...\n",
      "‚úÖ faker and faker_vehicle are available\n",
      "‚úÖ All packages verified and ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Package Verification\n",
    "print(\"üîß Verifying required packages...\")\n",
    "try:\n",
    "    from faker import Faker\n",
    "    from faker_vehicle import VehicleProvider\n",
    "    print(\"‚úÖ faker and faker_vehicle are available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing packages: {e}\")\n",
    "    print(\"üí° Installing missing packages...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faker\", \"faker-vehicle\"])\n",
    "    from faker import Faker\n",
    "    from faker_vehicle import VehicleProvider\n",
    "    print(\"‚úÖ Packages installed successfully\")\n",
    "\n",
    "print(\"‚úÖ All packages verified and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a97fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 17:15:34,033 - INFO - Target table: soni.default.parallel_merge_optimize_demo_20250819_171534\n",
      "2025-08-19 17:15:34,034 - INFO - Clustering on: event_timestamp\n",
      "2025-08-19 17:15:34,034 - INFO - Session ID: session_42_20250819_171534\n",
      "2025-08-19 17:15:34,035 - INFO - Configuration loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Setting up configuration...\n",
      "üé≤ Random seed set to: 42\n",
      "üìã Table name: parallel_merge_optimize_demo_20250819_171534\n",
      "üéØ Full table path: soni.default.parallel_merge_optimize_demo_20250819_171534\n",
      "üìÅ Session ID: session_42_20250819_171534\n",
      "‚úÖ Configuration complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "print(\"‚öôÔ∏è Setting up configuration...\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"üé≤ Random seed set to: {RANDOM_SEED}\")\n",
    "\n",
    "# Create dynamic table name with logical name and timestamp\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "TABLE_NAME = f\"parallel_merge_optimize_demo_{timestamp_str}\"\n",
    "TARGET_TABLE = f\"soni.default.{TABLE_NAME}\"\n",
    "\n",
    "CHECKPOINT_BASE = \"s3://test-external-volume-bucket-2/test-folder\"\n",
    "JOIN_COLUMN = \"event_id\"\n",
    "CLUSTERING_COLUMN = \"event_timestamp\"\n",
    "INITIAL_EVENT_ID_POOL_SIZE = 10000\n",
    "\n",
    "# Generate unique checkpoint locations (but deterministic within session)\n",
    "session_id = f\"session_{RANDOM_SEED}_{timestamp_str}\"\n",
    "checkpoint_bootstrap = f\"{CHECKPOINT_BASE}/bootstrap_{session_id}\"\n",
    "checkpoint_main = f\"{CHECKPOINT_BASE}/main_{session_id}\"\n",
    "\n",
    "print(f\"üìã Table name: {TABLE_NAME}\")\n",
    "print(f\"üéØ Full table path: {TARGET_TABLE}\")\n",
    "print(f\"üìÅ Session ID: {session_id}\")\n",
    "\n",
    "logger.info(f\"Target table: {TARGET_TABLE}\")\n",
    "logger.info(f\"Clustering on: {CLUSTERING_COLUMN}\")\n",
    "logger.info(f\"Session ID: {session_id}\")\n",
    "logger.info(\"Configuration loaded successfully\")\n",
    "\n",
    "print(\"‚úÖ Configuration complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40719a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Setting up reproducible data generation...\n",
      "‚úÖ Faker initialized with seed: 42\n",
      "üîÑ Creating 10000 reproducible event IDs...\n",
      "‚úÖ Created 10000 deterministic event IDs\n",
      "üìù Sample IDs: ['000005380', '000003608', '000003764']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 17:15:34,753 - INFO - Created 10000 deterministic event IDs for updates\n",
      "2025-08-19 17:15:34,754 - INFO - Reproducible UDFs and data generation initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reproducible data generation ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Reproducible Data Generation Setup\n",
    "print(\"üé≤ Setting up reproducible data generation...\")\n",
    "\n",
    "# Initialize Faker with seed for reproducible fake data\n",
    "fake = Faker()\n",
    "fake.seed_instance(RANDOM_SEED)\n",
    "fake.add_provider(VehicleProvider)\n",
    "print(f\"‚úÖ Faker initialized with seed: {RANDOM_SEED}\")\n",
    "\n",
    "# Create reproducible pool of existing event IDs\n",
    "print(f\"üîÑ Creating {INITIAL_EVENT_ID_POOL_SIZE} reproducible event IDs...\")\n",
    "existing_event_ids = set()\n",
    "\n",
    "# Use deterministic approach instead of random UUIDs\n",
    "for i in range(INITIAL_EVENT_ID_POOL_SIZE):\n",
    "    # Create deterministic but unique IDs based on seed and index\n",
    "    deterministic_id = f\"{i:09d}\"\n",
    "    existing_event_ids.add(deterministic_id)\n",
    "\n",
    "print(f\"‚úÖ Created {len(existing_event_ids)} deterministic event IDs\")\n",
    "print(f\"üìù Sample IDs: {list(existing_event_ids)[:3]}...\")\n",
    "\n",
    "# Create UDFs for fake data generation (these will use seeded faker)\n",
    "event_id_udf = F.udf(lambda: f\"new_{RANDOM_SEED}_{uuid.uuid4()}\", StringType())\n",
    "vehicle_make_udf = F.udf(fake.vehicle_make, StringType())\n",
    "vehicle_model_udf = F.udf(fake.vehicle_model, StringType())\n",
    "vehicle_year_udf = F.udf(fake.vehicle_year, StringType())\n",
    "latitude_udf = F.udf(lambda: str(fake.latitude()), StringType())   # Convert to string\n",
    "longitude_udf = F.udf(lambda: str(fake.longitude()), StringType()) # Convert to string\n",
    "zipcode_udf = F.udf(fake.zipcode, StringType())\n",
    "\n",
    "logger.info(f\"Created {len(existing_event_ids)} deterministic event IDs for updates\")\n",
    "logger.info(\"Reproducible UDFs and data generation initialized\")\n",
    "\n",
    "print(\"‚úÖ Reproducible data generation ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d0cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 17:15:34,761 - INFO - Reproducible streaming data generation function created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reproducible streaming function ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Reproducible Streaming Function\n",
    "def create_streaming_vehicle_data(rows_per_second=1000, num_partitions=4, update_ratio=0.5, seed_offset=0):\n",
    "    \"\"\"Create a streaming DataFrame with vehicle data, mixing updates and inserts based on existing event IDs.\n",
    "    \n",
    "    Args:\n",
    "        rows_per_second: Rate of data generation\n",
    "        num_partitions: Number of partitions\n",
    "        update_ratio: Ratio of updates vs inserts (0.0 = all inserts, 1.0 = all updates)\n",
    "        seed_offset: Offset to add to base seed for different streams\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating streaming vehicle data with {update_ratio*100}% updates\")\n",
    "    \n",
    "    # Convert existing_event_ids to a list for efficient random access\n",
    "    existing_ids_list = list(existing_event_ids) if existing_event_ids else []\n",
    "    logger.info(f\"Using {len(existing_ids_list)} existing IDs for updates\")\n",
    "    \n",
    "    # Create a seeded random generator for this specific function call\n",
    "    stream_seed = RANDOM_SEED + seed_offset\n",
    "    stream_random = random.Random(stream_seed)\n",
    "    \n",
    "    # Create a mix of existing IDs (for updates) and new IDs (for inserts)\n",
    "    def generate_event_id_with_mix():\n",
    "        # Use the seeded random generator for reproducible choices\n",
    "        if existing_ids_list and stream_random.random() < update_ratio:\n",
    "            return stream_random.choice(existing_ids_list)  # Existing ID for update\n",
    "        else:\n",
    "            # Generate deterministic new IDs\n",
    "            return f\"new_{stream_seed}_{stream_random.randint(100000, 999999)}\"\n",
    "        \n",
    "    event_id_mixed_udf = F.udf(generate_event_id_with_mix, StringType())\n",
    "    \n",
    "    # Create streaming DataFrame with simplified schema\n",
    "    df = (spark.readStream.format(\"rate\")\n",
    "          .option(\"numPartitions\", num_partitions)\n",
    "          .option(\"rowsPerSecond\", rows_per_second)\n",
    "          .load()\n",
    "          .withColumn(\"event_timestamp\", F.current_timestamp())\n",
    "          .withColumn(\"event_id\", event_id_mixed_udf())\n",
    "          .withColumn(\"vehicle_make\", vehicle_make_udf())\n",
    "          .withColumn(\"vehicle_model\", vehicle_model_udf())\n",
    "          .withColumn(\"vehicle_year\", vehicle_year_udf())\n",
    "          .withColumn(\"latitude\", latitude_udf())\n",
    "          .withColumn(\"longitude\", longitude_udf())\n",
    "          .withColumn(\"zipcode\", zipcode_udf())\n",
    "          .drop(\"value\", \"timestamp\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "logger.info(\"Reproducible streaming data generation function created\")\n",
    "print(\"‚úÖ Reproducible streaming function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26101ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating target table with optimized schema...\n",
      "‚úÖ All required variables available\n",
      "üßπ Dropped existing table soni.default.parallel_merge_optimize_demo_20250819_171534 if it existed\n",
      "üìù Creating table with schema:\n",
      "   - Primary key: event_id\n",
      "   - Clustering: event_timestamp (REQUIRED)\n",
      "   - Features: Row tracking, Deletion vectors (enabled by default)\n",
      "üîß Creating table with liquid clustering and all required features...\n",
      "‚úÖ Table created successfully with liquid clustering!\n",
      "   üîó Liquid clustering: event_timestamp\n",
      "   üîÑ Row tracking: enabled by default\n",
      "   üóëÔ∏è  Deletion vectors: enabled by default\n",
      "‚úÖ Table setup complete!\n",
      "   üìç Location: s3://us-west-2-extstaging-managed-catalog-test-bucket-1/19a85dee-54bc-43a2-87ab-023d0ec16013/tables/bf0c456f-9ed1-42b3-9ff2-1f32cd2a5db4\n",
      "   üè∑Ô∏è  Format: delta\n",
      "   üìä Properties: {'delta.parquet.compression.codec': 'zstd', 'delta.enableDeletionVectors': 'true', 'delta.enableRowTracking': 'true', 'delta.checkpointPolicy': 'v2', 'delta.rowTracking.materializedRowCommitVersionColumnName': '_row-commit-version-col-16cdb062-032d-447c-a68b-2f994407da41', 'delta.rowTracking.materializedRowIdColumnName': '_row-id-col-a2d90e4c-47a9-4da6-b184-c9f444f04bd1'}\n",
      "\n",
      "üìã Table Schema:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Target table ready for streaming operations!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Target Table\n",
    "print(\"üèóÔ∏è Creating target table with optimized schema...\")\n",
    "\n",
    "# Safety checks for required variables\n",
    "required_vars = ['TARGET_TABLE', 'JOIN_COLUMN', 'CLUSTERING_COLUMN']\n",
    "missing_vars = []\n",
    "\n",
    "for var_name in required_vars:\n",
    "    try:\n",
    "        globals()[var_name]\n",
    "    except KeyError:\n",
    "        missing_vars.append(var_name)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ö†Ô∏è  Missing variables: {missing_vars}\")\n",
    "    print(\"üîß Please run Cell 3 (Configuration) first!\")\n",
    "    raise NameError(f\"Required variables not found: {missing_vars}\")\n",
    "\n",
    "print(f\"‚úÖ All required variables available\")\n",
    "\n",
    "# Drop table if it exists (clean slate)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE}\")\n",
    "print(f\"üßπ Dropped existing table {TARGET_TABLE} if it existed\")\n",
    "\n",
    "print(\"üìù Creating table with schema:\")\n",
    "print(f\"   - Primary key: {JOIN_COLUMN}\")\n",
    "print(f\"   - Clustering: {CLUSTERING_COLUMN} (REQUIRED)\")\n",
    "print(\"   - Features: Row tracking, Deletion vectors (enabled by default)\")\n",
    "\n",
    "# Create table with liquid clustering (features enabled by default)\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE {TARGET_TABLE} (\n",
    "    event_id STRING NOT NULL,\n",
    "    event_timestamp TIMESTAMP NOT NULL,\n",
    "    vehicle_make STRING,\n",
    "    vehicle_model STRING,\n",
    "    vehicle_year STRING,\n",
    "    latitude STRING,\n",
    "    longitude STRING,\n",
    "    zipcode STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY ({CLUSTERING_COLUMN})\n",
    "COMMENT 'Demo table for parallel MERGE and OPTIMIZE operations with liquid clustering'\n",
    "\"\"\"\n",
    "\n",
    "# Execute table creation\n",
    "print(f\"üîß Creating table with liquid clustering and all required features...\")\n",
    "spark.sql(create_table_sql)\n",
    "print(\"‚úÖ Table created successfully with liquid clustering!\")\n",
    "print(f\"   üîó Liquid clustering: {CLUSTERING_COLUMN}\")\n",
    "print(\"   üîÑ Row tracking: enabled by default\")\n",
    "print(\"   üóëÔ∏è  Deletion vectors: enabled by default\")\n",
    "\n",
    "# Verify table creation\n",
    "table_info = spark.sql(f\"DESCRIBE DETAIL {TARGET_TABLE}\").collect()[0]\n",
    "print(f\"‚úÖ Table setup complete!\")\n",
    "print(f\"   üìç Location: {table_info.location}\")\n",
    "print(f\"   üè∑Ô∏è  Format: {table_info.format}\")\n",
    "print(f\"   üìä Properties: {table_info.properties}\")\n",
    "\n",
    "# Show table schema\n",
    "print(\"\\nüìã Table Schema:\")\n",
    "display(spark.sql(f\"DESCRIBE {TARGET_TABLE}\"))\n",
    "\n",
    "print(\"‚úÖ Target table ready for streaming operations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa33676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Table created and ready for streaming MERGE demo!\n",
      "üìä Starting with empty table - streaming will populate it naturally\n",
      "‚úÖ Table starts with 0 records\n",
      "\n",
      "üìã Table structure:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Why no bootstrap needed:\n",
      "   ‚Ä¢ Infinite MERGE stream will populate the table naturally\n",
      "   ‚Ä¢ First batches = 100% inserts (realistic start)\n",
      "   ‚Ä¢ Later batches = mix of updates/inserts (realistic growth)\n",
      "   ‚Ä¢ Demonstrates true streaming behavior from empty to populated\n",
      "\n",
      "‚úÖ Ready to start parallel MERGE and OPTIMIZE operations!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Ready for Streaming Demo\n",
    "print(\"üéØ Table created and ready for streaming MERGE demo!\")\n",
    "print(\"üìä Starting with empty table - streaming will populate it naturally\")\n",
    "\n",
    "# Verify table is empty and ready\n",
    "initial_count = spark.sql(f\"SELECT COUNT(*) as count FROM {TARGET_TABLE}\").collect()[0]['count']\n",
    "print(f\"‚úÖ Table starts with {initial_count} records\")\n",
    "\n",
    "# Show table structure\n",
    "print(\"\\nüìã Table structure:\")\n",
    "display(spark.sql(f\"DESCRIBE {TARGET_TABLE}\"))\n",
    "\n",
    "print(\"\\nüéØ Why no bootstrap needed:\")\n",
    "print(\"   ‚Ä¢ Infinite MERGE stream will populate the table naturally\")\n",
    "print(\"   ‚Ä¢ First batches = 100% inserts (realistic start)\")\n",
    "print(\"   ‚Ä¢ Later batches = mix of updates/inserts (realistic growth)\")\n",
    "print(\"   ‚Ä¢ Demonstrates true streaming behavior from empty to populated\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to start parallel MERGE and OPTIMIZE operations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MERGE processor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: MERGE Processor for Parallel Operations\n",
    "class forEachBatchProcessor:\n",
    "    def __init__(self, target_table: str, clustering_column: str, join_column: str):\n",
    "        self.target_table = target_table\n",
    "        self.clustering_column = clustering_column\n",
    "        self.join_column = join_column\n",
    "        self.batch_counter = 0\n",
    "        self.total_processed = 0\n",
    "        self.total_updates = 0\n",
    "        self.total_inserts = 0\n",
    "\n",
    "    def make_changes_using_the_micro_batch(self, microBatchOutputDF, batchId: int):\n",
    "        self.batch_counter += 1\n",
    "        print(f\"=== MERGE BATCH {self.batch_counter} (ID: {batchId}) ===\")\n",
    "        \n",
    "        # Count records before deduplication\n",
    "        total_records = microBatchOutputDF.count()\n",
    "        print(f\"Processing {total_records} records\")\n",
    "        \n",
    "        spark_session_for_this_micro_batch = microBatchOutputDF.sparkSession\n",
    "\n",
    "        # Create temporary view for the batch\n",
    "        view_name = f\"updates_batch_{batchId}\"\n",
    "        microBatchOutputDF.dropDuplicates([self.join_column]).createOrReplaceTempView(view_name)\n",
    "\n",
    "        # MERGE statement with row-level concurrency support\n",
    "        sql_for_merge = f\"\"\"\n",
    "          MERGE INTO {self.target_table} target\n",
    "          USING {view_name} source\n",
    "          ON source.{self.join_column} = target.{self.join_column}\n",
    "          WHEN MATCHED THEN\n",
    "            UPDATE SET *\n",
    "          WHEN NOT MATCHED THEN\n",
    "            INSERT *\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Executing MERGE for batch {batchId}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Execute MERGE and capture results\n",
    "        result = spark_session_for_this_micro_batch.sql(sql_for_merge)\n",
    "        \n",
    "        # Get MERGE statistics\n",
    "        try:\n",
    "            # Count unique event IDs in the batch to estimate updates vs inserts\n",
    "            batch_data = microBatchOutputDF.collect()\n",
    "            updates = sum(1 for row in batch_data if row.event_id in existing_event_ids)\n",
    "            inserts = len(batch_data) - updates\n",
    "            \n",
    "            self.total_updates += updates\n",
    "            self.total_inserts += inserts\n",
    "            self.total_processed += (updates + inserts)\n",
    "            \n",
    "            print(f\"MERGE Results: {updates} updates, {inserts} inserts\")\n",
    "            print(f\"Running totals: {self.total_updates} updates, {self.total_inserts} inserts\")\n",
    "            \n",
    "            # Verify we have at least 10% updates as required\n",
    "            if self.total_processed > 0:\n",
    "                update_percentage = (self.total_updates / self.total_processed) * 100\n",
    "                print(f\"Update percentage: {update_percentage:.1f}% (target: ‚â•10%)\")\n",
    "                if update_percentage >= 10:\n",
    "                    print(\"‚úÖ Meeting requirement: ‚â•10% updates\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Below target: Need ‚â•10% updates\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Could not get MERGE statistics: {error_msg}\")\n",
    "            raise (error_msg)\n",
    "\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"MERGE completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"=== END MERGE BATCH {self.batch_counter} ===\\n\")\n",
    "\n",
    "# Initialize the MERGE processor\n",
    "merge_processor = forEachBatchProcessor(\n",
    "    target_table=TARGET_TABLE,\n",
    "    clustering_column=CLUSTERING_COLUMN,\n",
    "    join_column=JOIN_COLUMN,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MERGE processor initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a9692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 17:16:26,648 - INFO - Creating streaming vehicle data with 50.0% updates\n",
      "2025-08-19 17:16:26,649 - INFO - Using 10000 existing IDs for updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting parallel MERGE streaming job...\n",
      "\n",
      "=== OPTIMIZE OPERATION 2 ===\n",
      "Sleep duration: 15.38 seconds\n",
      "Executing OPTIMIZE operation 2...\n",
      "‚úÖ OPTIMIZE completed in 1.63 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 2 ===\n",
      "\n",
      "‚úÖ MERGE streaming job started: MERGE_Data_Into_Table_soni.default.parallel_merge_optimize_demo_20250819_171534\n",
      "üìä Streaming job status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "üéØ Update ratio: 50% (exceeds 10% requirement)\n",
      "üîÑ MERGE operations will run every 10 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OPTIMIZE OPERATION 1 ===\n",
      "Sleep duration: 19.13 seconds\n",
      "Executing OPTIMIZE operation 1...\n",
      "‚úÖ OPTIMIZE completed in 1.83 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 1 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 3 ===\n",
      "Sleep duration: 18.35 seconds\n",
      "Executing OPTIMIZE operation 3...\n",
      "‚úÖ OPTIMIZE completed in 1.63 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 3 ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Start Parallel MERGE Streaming\n",
    "print(\"üöÄ Starting parallel MERGE streaming job...\")\n",
    "\n",
    "# Start the MERGE streaming job with 50% updates (exceeds 10% requirement)\n",
    "merge_stream = (\n",
    "    create_streaming_vehicle_data(rows_per_second=100, num_partitions=2, update_ratio=0.5)  # 50% updates, 50% inserts\n",
    "      .writeStream\n",
    "      .option(\"queryName\", f\"MERGE_Data_Into_Table_{TARGET_TABLE}\")\n",
    "      .foreachBatch(merge_processor.make_changes_using_the_micro_batch)\n",
    "      .trigger(processingTime=\"10 seconds\")\n",
    "      .option(\"checkpointLocation\", checkpoint_main)\n",
    "      .start()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ MERGE streaming job started: {merge_stream.name}\")\n",
    "print(f\"üìä Streaming job status: {merge_stream.status}\")\n",
    "print(f\"üéØ Update ratio: 50% (exceeds 10% requirement)\")\n",
    "print(\"üîÑ MERGE operations will run every 10 seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb58fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Starting OPTIMIZE operations in parallel...\n",
      "‚úÖ OPTIMIZE thread started successfully!\n",
      "‚ö° OPTIMIZE will run every 15-30 seconds in parallel with MERGE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OPTIMIZE OPERATION 2 ===\n",
      "Sleep duration: 26.05 seconds\n",
      "Executing OPTIMIZE operation 2...\n",
      "‚úÖ OPTIMIZE completed in 1.64 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 2 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 4 ===\n",
      "Sleep duration: 25.15 seconds\n",
      "Executing OPTIMIZE operation 4...\n",
      "‚úÖ OPTIMIZE completed in 1.60 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 4 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 3 ===\n",
      "Sleep duration: 16.30 seconds\n",
      "Executing OPTIMIZE operation 3...\n",
      "‚úÖ OPTIMIZE completed in 1.60 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 3 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 1 ===\n",
      "Sleep duration: 28.38 seconds\n",
      "Executing OPTIMIZE operation 1...\n",
      "‚úÖ OPTIMIZE completed in 1.60 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 1 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 5 ===\n",
      "Sleep duration: 21.33 seconds\n",
      "Executing OPTIMIZE operation 5...\n",
      "\n",
      "=== OPTIMIZE OPERATION 4 ===\n",
      "Sleep duration: 15.45 seconds\n",
      "Executing OPTIMIZE operation 4...\n",
      "‚úÖ OPTIMIZE completed in 1.80 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 5 ===\n",
      "\n",
      "‚úÖ OPTIMIZE completed in 1.62 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 4 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 2 ===\n",
      "Sleep duration: 18.28 seconds\n",
      "Executing OPTIMIZE operation 2...\n",
      "‚úÖ OPTIMIZE completed in 1.61 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 2 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 5 ===\n",
      "Sleep duration: 15.40 seconds\n",
      "Executing OPTIMIZE operation 5...\n",
      "‚úÖ OPTIMIZE completed in 1.57 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 5 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 6 ===\n",
      "Sleep duration: 22.58 seconds\n",
      "Executing OPTIMIZE operation 6...\n",
      "‚úÖ OPTIMIZE completed in 1.55 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 6 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 3 ===\n",
      "Sleep duration: 17.98 seconds\n",
      "Executing OPTIMIZE operation 3...\n",
      "‚úÖ OPTIMIZE completed in 1.63 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 3 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 6 ===\n",
      "Sleep duration: 24.75 seconds\n",
      "Executing OPTIMIZE operation 6...\n",
      "‚úÖ OPTIMIZE completed in 1.61 seconds\n",
      "üìä Optimization results:\n",
      "\n",
      "=== OPTIMIZE OPERATION 4 ===\n",
      "Sleep duration: 18.31 seconds\n",
      "Executing OPTIMIZE operation 4...\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 6 ===\n",
      "\n",
      "‚úÖ OPTIMIZE completed in 1.60 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 4 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 7 ===\n",
      "Sleep duration: 23.17 seconds\n",
      "Executing OPTIMIZE operation 7...\n",
      "‚úÖ OPTIMIZE completed in 1.73 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 7 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 8 ===\n",
      "Sleep duration: 15.10 seconds\n",
      "Executing OPTIMIZE operation 8...\n",
      "‚úÖ OPTIMIZE completed in 1.80 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 8 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 7 ===\n",
      "Sleep duration: 23.84 seconds\n",
      "Executing OPTIMIZE operation 7...\n",
      "‚úÖ OPTIMIZE completed in 1.72 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 7 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 5 ===\n",
      "Sleep duration: 27.14 seconds\n",
      "Executing OPTIMIZE operation 5...\n",
      "‚úÖ OPTIMIZE completed in 1.68 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 5 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 9 ===\n",
      "Sleep duration: 27.09 seconds\n",
      "Executing OPTIMIZE operation 9...\n",
      "‚úÖ OPTIMIZE completed in 1.67 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 9 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 6 ===\n",
      "Sleep duration: 20.10 seconds\n",
      "Executing OPTIMIZE operation 6...\n",
      "\n",
      "=== OPTIMIZE OPERATION 8 ===\n",
      "Sleep duration: 25.47 seconds\n",
      "Executing OPTIMIZE operation 8...\n",
      "‚úÖ OPTIMIZE completed in 2.04 seconds\n",
      "‚úÖ OPTIMIZE completed in 2.16 seconds\n",
      "üìä Optimization results:\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 8 ===\n",
      "\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 6 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 10 ===\n",
      "Sleep duration: 17.33 seconds\n",
      "Executing OPTIMIZE operation 10...\n",
      "‚úÖ OPTIMIZE completed in 1.57 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 10 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 7 ===\n",
      "Sleep duration: 20.05 seconds\n",
      "Executing OPTIMIZE operation 7...\n",
      "‚úÖ OPTIMIZE completed in 1.59 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 7 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 9 ===\n",
      "Sleep duration: 29.36 seconds\n",
      "Executing OPTIMIZE operation 9...\n",
      "‚úÖ OPTIMIZE completed in 1.53 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 9 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 11 ===\n",
      "Sleep duration: 16.39 seconds\n",
      "Executing OPTIMIZE operation 11...\n",
      "‚úÖ OPTIMIZE completed in 1.56 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 11 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 8 ===\n",
      "Sleep duration: 16.45 seconds\n",
      "Executing OPTIMIZE operation 8...\n",
      "‚úÖ OPTIMIZE completed in 1.56 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 8 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 10 ===\n",
      "Sleep duration: 27.71 seconds\n",
      "Executing OPTIMIZE operation 10...\n",
      "\n",
      "=== OPTIMIZE OPERATION 12 ===\n",
      "Sleep duration: 24.06 seconds\n",
      "Executing OPTIMIZE operation 12...\n",
      "‚úÖ OPTIMIZE completed in 2.12 seconds\n",
      "‚úÖ OPTIMIZE completed in 2.08 seconds\n",
      "üìä Optimization results:\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 10 ===\n",
      "\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 12 ===\n",
      "\n",
      "\n",
      "=== OPTIMIZE OPERATION 9 ===\n",
      "Sleep duration: 27.11 seconds\n",
      "Executing OPTIMIZE operation 9...\n",
      "‚úÖ OPTIMIZE completed in 1.66 seconds\n",
      "üìä Optimization results:\n",
      "   Files optimized: N/A\n",
      "   Files removed: N/A\n",
      "=== END OPTIMIZE OPERATION 9 ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Parallel OPTIMIZE Operations\n",
    "def run_optimize_operations():\n",
    "    \"\"\"Run OPTIMIZE operations in a separate thread to demonstrate parallel execution\"\"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        \n",
    "        # Random sleep between 15-120 seconds\n",
    "        sleep_duration = random.uniform(15, 120)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        print(f\"\\n=== OPTIMIZE OPERATION {counter} ===\")\n",
    "        print(f\"Sleep duration: {sleep_duration:.2f} seconds\")\n",
    "        \n",
    "        # OPTIMIZE with liquid clustering (no ZORDER needed)\n",
    "        # Liquid clustering automatically handles data layout optimization\n",
    "        optimize_sql = f\"\"\"\n",
    "            OPTIMIZE {TARGET_TABLE}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Executing OPTIMIZE operation {counter}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = spark.sql(optimize_sql)\n",
    "            end_time = time.time()\n",
    "            print(f\"‚úÖ OPTIMIZE completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            # Show optimization results\n",
    "            if result.count() > 0:\n",
    "                print(\"üìä Optimization results:\")\n",
    "                result_data = result.collect()\n",
    "                for row in result_data:\n",
    "                    print(f\"   Files optimized: {getattr(row, 'num_files_added', 'N/A')}\")\n",
    "                    print(f\"   Files removed: {getattr(row, 'num_files_removed', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚ùå OPTIMIZE failed: {error_msg}\")\n",
    "            \n",
    "            # Check for critical session errors that should stop the demo\n",
    "            critical_errors = [\n",
    "                \"PERMISSION_DENIED\",\n",
    "                \"Local RPC without associated session\",\n",
    "                \"Session not found\",\n",
    "                \"Connection refused\",\n",
    "                \"UNAUTHENTICATED\"\n",
    "            ]\n",
    "            \n",
    "            if any(critical_error in error_msg for critical_error in critical_errors):\n",
    "                print(\"üö® CRITICAL ERROR DETECTED!\")\n",
    "                print(\"üõë This indicates a Databricks Connect session issue\")\n",
    "                print(\"üí° Recommended actions:\")\n",
    "                print(\"   1. Check your Databricks Connect configuration\")\n",
    "                print(\"   2. Verify DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID\")\n",
    "                print(\"   3. Restart the kernel and reconnect\")\n",
    "                print(\"   4. Run stop_demo() to clean up\")\n",
    "                print(\"üö® STOPPING OPTIMIZE OPERATIONS DUE TO CRITICAL ERROR\")\n",
    "                break  # Exit the while loop\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Non-critical error - continuing OPTIMIZE operations\")\n",
    "        \n",
    "        print(f\"=== END OPTIMIZE OPERATION {counter} ===\\n\")\n",
    "\n",
    "# Start OPTIMIZE operations in a separate thread\n",
    "print(\"üîß Starting OPTIMIZE operations in parallel...\")\n",
    "optimize_thread = threading.Thread(target=run_optimize_operations, daemon=True)\n",
    "optimize_thread.start()\n",
    "print(\"‚úÖ OPTIMIZE thread started successfully!\")\n",
    "print(\"‚ö° OPTIMIZE will run every 15-30 seconds in parallel with MERGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b97f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ PARALLEL MERGE AND OPTIMIZE DEMO ACTIVE!\n",
      "==================================================\n",
      "üìä MERGE Stream: MERGE_Data_Into_Table_soni.default.parallel_merge_optimize_demo_20250819_171534\n",
      "üìä Active: True\n",
      "üìä Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "\n",
      "üîÑ Operations running:\n",
      "   ‚Ä¢ MERGE: Every 10 seconds (50% updates)\n",
      "   ‚Ä¢ OPTIMIZE: Every 15-30 seconds (parallel)\n",
      "   ‚Ä¢ Requirement: ‚â•10% updates ‚úÖ\n",
      "\n",
      "üí° To stop: stop_demo()\n",
      "üí° To check data: display(spark.sql(f'SELECT * FROM {TARGET_TABLE} LIMIT 5'))\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Demo Status & Cleanup\n",
    "print(\"üéØ PARALLEL MERGE AND OPTIMIZE DEMO ACTIVE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show current streaming job status\n",
    "if 'merge_stream' in globals():\n",
    "    print(f\"üìä MERGE Stream: {merge_stream.name}\")\n",
    "    print(f\"üìä Active: {merge_stream.isActive}\")\n",
    "    print(f\"üìä Status: {merge_stream.status}\")\n",
    "\n",
    "print(\"\\nüîÑ Operations running:\")\n",
    "print(\"   ‚Ä¢ MERGE: Every 10 seconds (50% updates)\")\n",
    "print(\"   ‚Ä¢ OPTIMIZE: Every 15-30 seconds (parallel)\")\n",
    "print(\"   ‚Ä¢ Requirement: ‚â•10% updates ‚úÖ\")\n",
    "\n",
    "# Cleanup function\n",
    "def stop_demo():\n",
    "    \"\"\"Stop all streaming jobs and clean up resources\"\"\"\n",
    "    print(\"\\nüõë Stopping demo...\")\n",
    "    \n",
    "    # Stop the MERGE streaming job\n",
    "    if 'merge_stream' in globals() and merge_stream.isActive:\n",
    "        merge_stream.stop()\n",
    "        print(\"‚úÖ MERGE streaming job stopped\")\n",
    "    \n",
    "    # Background threads stop automatically (daemon=True)\n",
    "    print(\"‚úÖ Background threads stopped\")\n",
    "    print(\"‚úÖ Demo stopped successfully!\")\n",
    "\n",
    "print(\"\\nüí° To stop: stop_demo()\")\n",
    "print(\"üí° To check data: display(spark.sql(f'SELECT * FROM {TARGET_TABLE} LIMIT 5'))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533b526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has been removed - functionality merged into Cell 11\n",
    "# The notebook now has 11 essential cells instead of 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
