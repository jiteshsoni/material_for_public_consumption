{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030f2b3a-3895-47b3-8326-b193dba254cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MERGE and OPTIMIZE Parallel Demo with Databricks Connect\n",
    "\n",
    "This notebook demonstrates that MERGE and OPTIMIZE can run in parallel with row-level concurrency, deletion vectors, and liquid clustering in Databricks.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Connect configured locally\n",
    "- Environment variables set: DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID\n",
    "\n",
    "## Overview\n",
    "\n",
    "The demo creates a Delta table with the following features:\n",
    "- **Row-level concurrency**: Multiple writers can modify different rows simultaneously\n",
    "- **Deletion vectors**: Efficient deletion without rewriting entire files\n",
    "- **Liquid clustering**: Automatic clustering on the merge column for optimal performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8912ef1b-9cfa-411e-8dd0-1d0510e83638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Connection & Setup\n",
    "# üö® IMPORTANT: If you get version errors, RESTART THE KERNEL first!\n",
    "\n",
    "import sys\n",
    "print(f\"üêç Python executable: {sys.executable}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "\n",
    "# Initialize Databricks Connect\n",
    "print(\"üîó Initializing Databricks Connect...\")\n",
    "try:\n",
    "    from databricks.connect import DatabricksSession\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    print(\"‚úÖ Connected to Databricks cluster via Databricks Connect\")\n",
    "    \n",
    "    # Verify connection\n",
    "    print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "    test_count = spark.range(3).count()\n",
    "    print(f\"‚úÖ Connected to remote cluster: {test_count} test rows\")\n",
    "    \n",
    "    # Check Databricks Runtime version\n",
    "    try:\n",
    "        version_info = spark.sql(\"SELECT current_version() as version\").collect()[0]\n",
    "        print(f\"üè¢ Databricks Runtime: {version_info.version}\")\n",
    "        print(\"‚úÖ CONFIRMED: Running on DATABRICKS CLUSTER\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not get runtime version: {str(e)[:50]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"üí° Try restarting the kernel and running again\")\n",
    "    raise\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from faker import Faker\n",
    "from faker_vehicle import VehicleProvider\n",
    "import uuid\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All libraries imported and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc50a821-5c27-4d64-a397-04723012ed4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Package Verification\n",
    "print(\"üîß Verifying required packages...\")\n",
    "try:\n",
    "    from faker import Faker\n",
    "    from faker_vehicle import VehicleProvider\n",
    "    print(\"‚úÖ faker and faker_vehicle are available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing packages: {e}\")\n",
    "    print(\"üí° Installing missing packages...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faker\", \"faker-vehicle\"])\n",
    "    from faker import Faker\n",
    "    from faker_vehicle import VehicleProvider\n",
    "    print(\"‚úÖ Packages installed successfully\")\n",
    "\n",
    "print(\"‚úÖ All packages verified and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aafde8a-4020-4efc-a4fa-2fbb71ce2e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "print(\"‚öôÔ∏è Setting up configuration...\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"üé≤ Random seed set to: {RANDOM_SEED}\")\n",
    "\n",
    "# Create dynamic table name with logical name and timestamp\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "TABLE_NAME = f\"parallel_merge_optimize_demo_{timestamp_str}\"\n",
    "TARGET_TABLE = f\"soni.default.{TABLE_NAME}\"\n",
    "\n",
    "CHECKPOINT_BASE = \"s3://test-external-volume-bucket-2/test-folder\"\n",
    "JOIN_COLUMN = \"event_id\"\n",
    "CLUSTERING_COLUMN = \"event_timestamp\"\n",
    "INITIAL_EVENT_ID_POOL_SIZE = 10000\n",
    "\n",
    "# Generate unique checkpoint locations (but deterministic within session)\n",
    "session_id = f\"session_{RANDOM_SEED}_{timestamp_str}\"\n",
    "checkpoint_bootstrap = f\"{CHECKPOINT_BASE}/bootstrap_{session_id}\"\n",
    "checkpoint_main = f\"{CHECKPOINT_BASE}/main_{session_id}\"\n",
    "\n",
    "print(f\"üìã Table name: {TABLE_NAME}\")\n",
    "print(f\"üéØ Full table path: {TARGET_TABLE}\")\n",
    "print(f\"üìÅ Session ID: {session_id}\")\n",
    "\n",
    "logger.info(f\"Target table: {TARGET_TABLE}\")\n",
    "logger.info(f\"Clustering on: {CLUSTERING_COLUMN}\")\n",
    "logger.info(f\"Session ID: {session_id}\")\n",
    "logger.info(\"Configuration loaded successfully\")\n",
    "\n",
    "print(\"‚úÖ Configuration complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ad7bef-1e0c-4bc2-86b0-fe82824d4dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Reproducible Data Generation Setup\n",
    "print(\"üé≤ Setting up reproducible data generation...\")\n",
    "\n",
    "# Initialize Faker with seed for reproducible fake data\n",
    "fake = Faker()\n",
    "fake.seed_instance(RANDOM_SEED)\n",
    "fake.add_provider(VehicleProvider)\n",
    "print(f\"‚úÖ Faker initialized with seed: {RANDOM_SEED}\")\n",
    "\n",
    "# Create reproducible pool of existing event IDs\n",
    "print(f\"üîÑ Creating {INITIAL_EVENT_ID_POOL_SIZE} reproducible event IDs...\")\n",
    "existing_event_ids = set()\n",
    "\n",
    "# Use deterministic approach instead of random UUIDs\n",
    "for i in range(INITIAL_EVENT_ID_POOL_SIZE):\n",
    "    # Create deterministic but unique IDs based on seed and index\n",
    "    deterministic_id = f\"{i:09d}\"\n",
    "    existing_event_ids.add(deterministic_id)\n",
    "\n",
    "print(f\"‚úÖ Created {len(existing_event_ids)} deterministic event IDs\")\n",
    "print(f\"üìù Sample IDs: {list(existing_event_ids)[:3]}...\")\n",
    "\n",
    "# Create UDFs for fake data generation (these will use seeded faker)\n",
    "event_id_udf = F.udf(lambda: f\"new_{RANDOM_SEED}_{uuid.uuid4()}\", StringType())\n",
    "vehicle_make_udf = F.udf(fake.vehicle_make, StringType())\n",
    "vehicle_model_udf = F.udf(fake.vehicle_model, StringType())\n",
    "vehicle_year_udf = F.udf(fake.vehicle_year, StringType())\n",
    "latitude_udf = F.udf(lambda: str(fake.latitude()), StringType())   # Convert to string\n",
    "longitude_udf = F.udf(lambda: str(fake.longitude()), StringType()) # Convert to string\n",
    "zipcode_udf = F.udf(fake.zipcode, StringType())\n",
    "\n",
    "logger.info(f\"Created {len(existing_event_ids)} deterministic event IDs for updates\")\n",
    "logger.info(\"Reproducible UDFs and data generation initialized\")\n",
    "\n",
    "print(\"‚úÖ Reproducible data generation ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd5e87c-a000-43b4-9fa0-1ede7aa48699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Reproducible Streaming Function\n",
    "def create_streaming_vehicle_data(rows_per_second=1000, num_partitions=4, update_ratio=0.5, seed_offset=0):\n",
    "    \"\"\"Create a streaming DataFrame with vehicle data, mixing updates and inserts based on existing event IDs.\n",
    "    \n",
    "    Args:\n",
    "        rows_per_second: Rate of data generation\n",
    "        num_partitions: Number of partitions\n",
    "        update_ratio: Ratio of updates vs inserts (0.0 = all inserts, 1.0 = all updates)\n",
    "        seed_offset: Offset to add to base seed for different streams\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating streaming vehicle data with {update_ratio*100}% updates\")\n",
    "    \n",
    "    # Convert existing_event_ids to a list for efficient random access\n",
    "    existing_ids_list = list(existing_event_ids) if existing_event_ids else []\n",
    "    logger.info(f\"Using {len(existing_ids_list)} existing IDs for updates\")\n",
    "    \n",
    "    # Create a seeded random generator for this specific function call\n",
    "    stream_seed = RANDOM_SEED + seed_offset\n",
    "    stream_random = random.Random(stream_seed)\n",
    "    \n",
    "    # Create a mix of existing IDs (for updates) and new IDs (for inserts)\n",
    "    def generate_event_id_with_mix():\n",
    "        # Use the seeded random generator for reproducible choices\n",
    "        if existing_ids_list and stream_random.random() < update_ratio:\n",
    "            return stream_random.choice(existing_ids_list)  # Existing ID for update\n",
    "        else:\n",
    "            # Generate deterministic new IDs\n",
    "            return f\"new_{stream_seed}_{stream_random.randint(100000, 999999)}\"\n",
    "        \n",
    "    event_id_mixed_udf = F.udf(generate_event_id_with_mix, StringType())\n",
    "    \n",
    "    # Create streaming DataFrame with simplified schema\n",
    "    df = (spark.readStream.format(\"rate\")\n",
    "          .option(\"numPartitions\", num_partitions)\n",
    "          .option(\"rowsPerSecond\", rows_per_second)\n",
    "          .load()\n",
    "          .withColumn(\"event_timestamp\", F.current_timestamp())\n",
    "          .withColumn(\"event_id\", event_id_mixed_udf())\n",
    "          .withColumn(\"vehicle_make\", vehicle_make_udf())\n",
    "          .withColumn(\"vehicle_model\", vehicle_model_udf())\n",
    "          .withColumn(\"vehicle_year\", vehicle_year_udf())\n",
    "          .withColumn(\"latitude\", latitude_udf())\n",
    "          .withColumn(\"longitude\", longitude_udf())\n",
    "          .withColumn(\"zipcode\", zipcode_udf())\n",
    "          .drop(\"value\", \"timestamp\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "logger.info(\"Reproducible streaming data generation function created\")\n",
    "print(\"‚úÖ Reproducible streaming function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebe9ce1-6183-43dd-9f6f-e615206e638c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Create Target Table\n",
    "print(\"üèóÔ∏è Creating target table with optimized schema...\")\n",
    "\n",
    "# Safety checks for required variables\n",
    "required_vars = ['TARGET_TABLE', 'JOIN_COLUMN', 'CLUSTERING_COLUMN']\n",
    "missing_vars = []\n",
    "\n",
    "for var_name in required_vars:\n",
    "    try:\n",
    "        globals()[var_name]\n",
    "    except KeyError:\n",
    "        missing_vars.append(var_name)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ö†Ô∏è  Missing variables: {missing_vars}\")\n",
    "    print(\"üîß Please run Cell 3 (Configuration) first!\")\n",
    "    raise NameError(f\"Required variables not found: {missing_vars}\")\n",
    "\n",
    "print(f\"‚úÖ All required variables available\")\n",
    "\n",
    "# Drop table if it exists (clean slate)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE}\")\n",
    "print(f\"üßπ Dropped existing table {TARGET_TABLE} if it existed\")\n",
    "\n",
    "print(\"üìù Creating table with schema:\")\n",
    "print(f\"   - Primary key: {JOIN_COLUMN}\")\n",
    "print(f\"   - Clustering: {CLUSTERING_COLUMN} (REQUIRED)\")\n",
    "print(\"   - Features: Row tracking, Deletion vectors (enabled by default)\")\n",
    "\n",
    "# Create table with liquid clustering (features enabled by default)\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE {TARGET_TABLE} (\n",
    "    event_id STRING NOT NULL,\n",
    "    event_timestamp TIMESTAMP NOT NULL,\n",
    "    vehicle_make STRING,\n",
    "    vehicle_model STRING,\n",
    "    vehicle_year STRING,\n",
    "    latitude STRING,\n",
    "    longitude STRING,\n",
    "    zipcode STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY ({CLUSTERING_COLUMN})\n",
    "COMMENT 'Demo table for parallel MERGE and OPTIMIZE operations with liquid clustering'\n",
    "\"\"\"\n",
    "\n",
    "# Execute table creation\n",
    "print(f\"üîß Creating table with liquid clustering and all required features...\")\n",
    "spark.sql(create_table_sql)\n",
    "print(\"‚úÖ Table created successfully with liquid clustering!\")\n",
    "print(f\"   üîó Liquid clustering: {CLUSTERING_COLUMN}\")\n",
    "print(\"   üîÑ Row tracking: enabled by default\")\n",
    "print(\"   üóëÔ∏è  Deletion vectors: enabled by default\")\n",
    "\n",
    "# Verify table creation\n",
    "table_info = spark.sql(f\"DESCRIBE DETAIL {TARGET_TABLE}\").collect()[0]\n",
    "print(f\"‚úÖ Table setup complete!\")\n",
    "print(f\"   üìç Location: {table_info.location}\")\n",
    "print(f\"   üè∑Ô∏è  Format: {table_info.format}\")\n",
    "print(f\"   üìä Properties: {table_info.properties}\")\n",
    "\n",
    "# Show table schema\n",
    "print(\"\\nüìã Table Schema:\")\n",
    "display(spark.sql(f\"DESCRIBE {TARGET_TABLE}\"))\n",
    "\n",
    "print(\"‚úÖ Target table ready for streaming operations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40339aef-a5c7-495e-bd72-6d48015898f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Ready for Streaming Demo\n",
    "print(\"üéØ Table created and ready for streaming MERGE demo!\")\n",
    "print(\"üìä Starting with empty table - streaming will populate it naturally\")\n",
    "\n",
    "# Verify table is empty and ready\n",
    "initial_count = spark.sql(f\"SELECT COUNT(*) as count FROM {TARGET_TABLE}\").collect()[0]['count']\n",
    "print(f\"‚úÖ Table starts with {initial_count} records\")\n",
    "\n",
    "# Show table structure\n",
    "print(\"\\nüìã Table structure:\")\n",
    "display(spark.sql(f\"DESCRIBE {TARGET_TABLE}\"))\n",
    "\n",
    "print(\"\\nüéØ Why no bootstrap needed:\")\n",
    "print(\"   ‚Ä¢ Infinite MERGE stream will populate the table naturally\")\n",
    "print(\"   ‚Ä¢ First batches = 100% inserts (realistic start)\")\n",
    "print(\"   ‚Ä¢ Later batches = mix of updates/inserts (realistic growth)\")\n",
    "print(\"   ‚Ä¢ Demonstrates true streaming behavior from empty to populated\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to start parallel MERGE and OPTIMIZE operations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff10eb0-c5a0-481d-ba32-a8ebc6c21a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: MERGE Processor for Parallel Operations\n",
    "class forEachBatchProcessor:\n",
    "    def __init__(self, target_table: str, clustering_column: str, join_column: str):\n",
    "        self.target_table = target_table\n",
    "        self.clustering_column = clustering_column\n",
    "        self.join_column = join_column\n",
    "        self.batch_counter = 0\n",
    "        self.total_processed = 0\n",
    "        self.total_updates = 0\n",
    "        self.total_inserts = 0\n",
    "\n",
    "    def make_changes_using_the_micro_batch(self, microBatchOutputDF, batchId: int):\n",
    "        self.batch_counter += 1\n",
    "        print(f\"=== MERGE BATCH {self.batch_counter} (ID: {batchId}) ===\")\n",
    "        \n",
    "        # Count records before deduplication\n",
    "        total_records = microBatchOutputDF.count()\n",
    "        print(f\"Processing {total_records} records\")\n",
    "        \n",
    "        spark_session_for_this_micro_batch = microBatchOutputDF.sparkSession\n",
    "\n",
    "        # Create temporary view for the batch\n",
    "        view_name = f\"updates_batch_{batchId}\"\n",
    "        microBatchOutputDF.dropDuplicates([self.join_column]).createOrReplaceTempView(view_name)\n",
    "\n",
    "        # MERGE statement with row-level concurrency support\n",
    "        sql_for_merge = f\"\"\"\n",
    "          MERGE INTO {self.target_table} target\n",
    "          USING {view_name} source\n",
    "          ON source.{self.join_column} = target.{self.join_column}\n",
    "          WHEN MATCHED THEN\n",
    "            UPDATE SET *\n",
    "          WHEN NOT MATCHED THEN\n",
    "            INSERT *\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Executing MERGE for batch {batchId}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Execute MERGE and capture results\n",
    "        result = spark_session_for_this_micro_batch.sql(sql_for_merge)\n",
    "        \n",
    "        # Get MERGE statistics\n",
    "        try:\n",
    "            # Count unique event IDs in the batch to estimate updates vs inserts\n",
    "            batch_data = microBatchOutputDF.collect()\n",
    "            updates = sum(1 for row in batch_data if row.event_id in existing_event_ids)\n",
    "            inserts = len(batch_data) - updates\n",
    "            \n",
    "            self.total_updates += updates\n",
    "            self.total_inserts += inserts\n",
    "            self.total_processed += (updates + inserts)\n",
    "            \n",
    "            print(f\"MERGE Results: {updates} updates, {inserts} inserts\")\n",
    "            print(f\"Running totals: {self.total_updates} updates, {self.total_inserts} inserts\")\n",
    "            \n",
    "            # Verify we have at least 10% updates as required\n",
    "            if self.total_processed > 0:\n",
    "                update_percentage = (self.total_updates / self.total_processed) * 100\n",
    "                print(f\"Update percentage: {update_percentage:.1f}% (target: ‚â•10%)\")\n",
    "                if update_percentage >= 10:\n",
    "                    print(\"‚úÖ Meeting requirement: ‚â•10% updates\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Below target: Need ‚â•10% updates\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Could not get MERGE statistics: {error_msg}\")\n",
    "            raise (error_msg)\n",
    "\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"MERGE completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"=== END MERGE BATCH {self.batch_counter} ===\\n\")\n",
    "\n",
    "# Initialize the MERGE processor\n",
    "merge_processor = forEachBatchProcessor(\n",
    "    target_table=TARGET_TABLE,\n",
    "    clustering_column=CLUSTERING_COLUMN,\n",
    "    join_column=JOIN_COLUMN,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MERGE processor initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf1de2f-2670-4a99-a8d7-71871dc6ba50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: Start Parallel MERGE Streaming\n",
    "print(\"üöÄ Starting parallel MERGE streaming job...\")\n",
    "\n",
    "# Start the MERGE streaming job with 50% updates (exceeds 10% requirement)\n",
    "merge_stream = (\n",
    "    create_streaming_vehicle_data(rows_per_second=100, num_partitions=2, update_ratio=0.5)  # 50% updates, 50% inserts\n",
    "      .writeStream\n",
    "      .option(\"queryName\", f\"MERGE_Data_Into_Table_{TARGET_TABLE}\")\n",
    "      .foreachBatch(merge_processor.make_changes_using_the_micro_batch)\n",
    "      .trigger(processingTime=\"10 seconds\")\n",
    "      .option(\"checkpointLocation\", checkpoint_main)\n",
    "      .start()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ MERGE streaming job started: {merge_stream.name}\")\n",
    "print(f\"üìä Streaming job status: {merge_stream.status}\")\n",
    "print(f\"üéØ Update ratio: 50% (exceeds 10% requirement)\")\n",
    "print(\"üîÑ MERGE operations will run every 10 seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81514b08-aee9-49e9-8838-e58eb596ea17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Parallel OPTIMIZE Operations\n",
    "def run_optimize_operations():\n",
    "    \"\"\"Run OPTIMIZE operations in a separate thread to demonstrate parallel execution\"\"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        \n",
    "        # Random sleep between 15-120 seconds\n",
    "        sleep_duration = random.uniform(15, 120)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        print(f\"\\n=== OPTIMIZE OPERATION {counter} ===\")\n",
    "        print(f\"Sleep duration: {sleep_duration:.2f} seconds\")\n",
    "        \n",
    "        # OPTIMIZE with liquid clustering (no ZORDER needed)\n",
    "        # Liquid clustering automatically handles data layout optimization\n",
    "        optimize_sql = f\"\"\"\n",
    "            OPTIMIZE {TARGET_TABLE}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Executing OPTIMIZE operation {counter}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = spark.sql(optimize_sql)\n",
    "            end_time = time.time()\n",
    "            print(f\"‚úÖ OPTIMIZE completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            # Show optimization results\n",
    "            if result.count() > 0:\n",
    "                print(\"üìä Optimization results:\")\n",
    "                result_data = result.collect()\n",
    "                for row in result_data:\n",
    "                    print(f\"   Files optimized: {getattr(row, 'num_files_added', 'N/A')}\")\n",
    "                    print(f\"   Files removed: {getattr(row, 'num_files_removed', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚ùå OPTIMIZE failed: {error_msg}\")\n",
    "            \n",
    "            # Check for critical session errors that should stop the demo\n",
    "            critical_errors = [\n",
    "                \"PERMISSION_DENIED\",\n",
    "                \"Local RPC without associated session\",\n",
    "                \"Session not found\",\n",
    "                \"Connection refused\",\n",
    "                \"UNAUTHENTICATED\"\n",
    "            ]\n",
    "            \n",
    "            if any(critical_error in error_msg for critical_error in critical_errors):\n",
    "                print(\"üö® CRITICAL ERROR DETECTED!\")\n",
    "                print(\"üõë This indicates a Databricks Connect session issue\")\n",
    "                print(\"üí° Recommended actions:\")\n",
    "                print(\"   1. Check your Databricks Connect configuration\")\n",
    "                print(\"   2. Verify DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID\")\n",
    "                print(\"   3. Restart the kernel and reconnect\")\n",
    "                print(\"   4. Run stop_demo() to clean up\")\n",
    "                print(\"üö® STOPPING OPTIMIZE OPERATIONS DUE TO CRITICAL ERROR\")\n",
    "                break  # Exit the while loop\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Non-critical error - continuing OPTIMIZE operations\")\n",
    "        \n",
    "        print(f\"=== END OPTIMIZE OPERATION {counter} ===\\n\")\n",
    "\n",
    "# Start OPTIMIZE operations in a separate thread\n",
    "print(\"üîß Starting OPTIMIZE operations in parallel...\")\n",
    "optimize_thread = threading.Thread(target=run_optimize_operations, daemon=True)\n",
    "optimize_thread.start()\n",
    "print(\"‚úÖ OPTIMIZE thread started successfully!\")\n",
    "print(\"‚ö° OPTIMIZE will run every 15-30 seconds in parallel with MERGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf2d8a9-d4fa-4ea8-abfc-2aeca7ffc9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Demo Status & Cleanup\n",
    "print(\"üéØ PARALLEL MERGE AND OPTIMIZE DEMO ACTIVE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show current streaming job status\n",
    "if 'merge_stream' in globals():\n",
    "    print(f\"üìä MERGE Stream: {merge_stream.name}\")\n",
    "    print(f\"üìä Active: {merge_stream.isActive}\")\n",
    "    print(f\"üìä Status: {merge_stream.status}\")\n",
    "\n",
    "print(\"\\nüîÑ Operations running:\")\n",
    "print(\"   ‚Ä¢ MERGE: Every 10 seconds (50% updates)\")\n",
    "print(\"   ‚Ä¢ OPTIMIZE: Every 15-30 seconds (parallel)\")\n",
    "print(\"   ‚Ä¢ Requirement: ‚â•10% updates ‚úÖ\")\n",
    "\n",
    "# Cleanup function\n",
    "def stop_demo():\n",
    "    \"\"\"Stop all streaming jobs and clean up resources\"\"\"\n",
    "    print(\"\\nüõë Stopping demo...\")\n",
    "    \n",
    "    # Stop the MERGE streaming job\n",
    "    if 'merge_stream' in globals() and merge_stream.isActive:\n",
    "        merge_stream.stop()\n",
    "        print(\"‚úÖ MERGE streaming job stopped\")\n",
    "    \n",
    "    # Background threads stop automatically (daemon=True)\n",
    "    print(\"‚úÖ Background threads stopped\")\n",
    "    print(\"‚úÖ Demo stopped successfully!\")\n",
    "\n",
    "print(\"\\nüí° To stop: stop_demo()\")\n",
    "print(\"üí° To check data: display(spark.sql(f'SELECT * FROM {TARGET_TABLE} LIMIT 5'))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb92479-5582-4b48-b532-fa7166f87aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell has been removed - functionality merged into Cell 11\n",
    "# The notebook now has 11 essential cells instead of 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cd3618-3530-4559-bf96-ee3a94d76ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "merge_and_optimize_parallel_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
